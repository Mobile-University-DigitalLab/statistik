
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Entscheidungsbäume &#8212; Modul Statistik und Machine Learning Modelle</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css?v=342acfa5" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Kapitel10/01_DecisionTrees';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bagging und Boosting bei Entscheidungsbäumen" href="02_BaggingBoosting.html" />
    <link rel="prev" title="Einfaches logistisches Regressionsmodell" href="../Aufgaben/Kapitel09/logregmodell.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/srh_logo.png" class="logo__image only-light" alt="Modul Statistik und Machine Learning Modelle - Home"/>
    <img src="../_static/srh_logo.png" class="logo__image only-dark pst-js-only" alt="Modul Statistik und Machine Learning Modelle - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Modul Statistik und Machine Learning Modelle
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Vorwort</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Vorwort.html">Vorwort</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Einführung in die Statistik</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Lernziele/Kapitel1_Lernziele.html">Lernziele</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel01/01_Deskriptive_Statistik.html">Deskriptive Statistik</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel01/02_Ma%C3%9Fe_der_zentralen_Tendenz.html">Maße der zentralen Tendenz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel01/03_Streuungsma%C3%9Fe.html">Streuungsmaße</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel01/04_Ma%C3%9Fe_der_Position.html">Das Positionsmaß</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel01/05_Ma%C3%9Fe_der_Relation_zwischen_Variablen.html">Maße für die Relation zwischen Variablen</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Aufgaben/Kapitel01/Aufgaben.html">Übungsaufgaben</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel01/pandas.html">Die Pandas Bibliothek</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel01/Pandas_Dataframes.html">Übung zu Pandas Dataframes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel01/Fuenf_Punkte.html">Maße der zentralen Tendenz, Streumaße und Fünf-Punkte Zusammenfassung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel01/Arithmetisches_Mittel.html">Arithmetisches Mittel</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Diskrete Zufallsvariablen</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Lernziele/Kapitel2_Lernziele.html">Lernziele</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel02/01_Diskrete_Zufallsvariablen.html">Diskrete Zufallsvariablen und ihre Wahrscheinlichkeitsverteilungen</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel02/02_Die_Binomialverteilung.html">Die Binomialverteilung</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel02/03_Die_Poisson_Verteilung.html">Die Poisson-Verteilung</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Aufgaben/Kapitel02/Aufgaben.html">Übungsaufgaben</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel02/Hewert.html">Häufigkeiten und Erwartungswert</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel02/Normierung.html">Normierung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel02/binomial.html">Binomialverteilung</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Stetige Zufallsvariablen</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Lernziele/Kapitel3_Lernziele.html">Lernziele</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel03/01_stetige_Zufallsvariablen.html">Stetige Zufallsvariablen und ihre Wahrscheinlichkeitsverteilungen</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel03/02_Die%20Normalverteilung.html">Die Normalverteilung</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel03/03_Die_kontinuierliche_gleichm%C3%A4%C3%9Fige_Verteilung.html">Die kontinuierliche gleichmäßige Verteilung</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel03/04_Die_Student_t_Verteilung.html">Die Student t-Verteilung</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel03/05_Die_Chi_Quadrat_Verteilung.html">Die Chi-Quadrat-Verteilung</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel03/06_Die_F_Verteilung.html">Die F-Verteilung</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Aufgaben/Kapitel03/Aufgaben.html">Übungsaufgaben</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel03/PDF.html">Wahrscheinlichkeitsdichtefunktion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel03/689599_Regel.html">Die 68-95-99,7-Regel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel03/Normalverteilung.html">Die Normalverteilung</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Der Zentrale Grenzwertsatz</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Lernziele/Kapitel4_Lernziele.html">Lernziele</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel04/01_Zentraler_Grenzwertsatz.html">Der zentrale Grenzwertsatz</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Aufgaben/Kapitel04/Aufgaben.html">Übungsaufgaben</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel04/CLT.html">Der Zentrale Grenzwertsatz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel04/Schaetzfehler.html">Stichprobenfehler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel04/Wuerfel.html">Würfelexperiment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel04/Test_auf_Normalverteilung.html">Test auf Normalverteilung</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Inferenzstatistik und Konfidenzintervalle</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Lernziele/Kapitel5_Lernziele.html">Lernziele</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel05/01_Inferenzstatistik_und_Konfidenzintervalle.html">Inferenzstatistik und Konfidenzintervalle</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Aufgaben/Kapitel05/Aufgaben.html">Übungsaufgaben</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel05/Konfidenzintervall.html">Konfidenzintervall</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel05/Punkschaetzer.html">Punktschätzungen bei unbekanntem <span class="math notranslate nohighlight">\(\sigma\)</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel05/Punkt_Intervall.html">Punkt- und Intervallschätzungen</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Hypothesentests</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Lernziele/Kapitel6_Lernziele.html">Lernziele</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel06/01_Hypothesentests.html">Hypothesentests</a></li>




<li class="toctree-l1"><a class="reference internal" href="../Kapitel06/02_Hypothesentests_Mittelwert_einer_Grundgesamtheit.html">Hypothesentests für den Mittelwert einer Grundgesamtheit</a></li>


<li class="toctree-l1"><a class="reference internal" href="../Kapitel06/03_Hypothesentests_zwei_Grundgesamtheitsmittelwerte.html">Hypothesentests für zwei Grundgesamtheitsmittelwerte</a></li>



<li class="toctree-l1"><a class="reference internal" href="../Kapitel06/04_Standardabweichung_der_Grundgesamtheit.html">Inferenz für die Standardabweichung der Grundgesamtheit</a></li>


<li class="toctree-l1"><a class="reference internal" href="../Kapitel06/05_Chi_Quadrat_Tests.html">Chi-Quadrat-Tests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel06/06_Inferenz_Regression_und_Korrelation.html">Inferenzmethoden in Regression und Korrelation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel06/07_Wahrscheinlichkeitstabellen.html">Wahrscheinlichkeits-Tabellen</a></li>




<li class="toctree-l1 has-children"><a class="reference internal" href="../Aufgaben/Kapitel06/Aufgaben.html">Übungsaufgaben</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel06/Fehler_1ter_2ter_Art.html">Fehler <span class="math notranslate nohighlight">\(1\)</span>-ter und <span class="math notranslate nohighlight">\(2\)</span>-ter Art</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel06/hypo_unabhSt.html">Hypothesentest - unabhängige Stichproben, <span class="math notranslate nohighlight">\(\sigma_1 \approx \sigma_2\)</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel06/pooled_t.html"><span class="math notranslate nohighlight">\(2\)</span>-Stichproben <span class="math notranslate nohighlight">\(t\)</span>-Test</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Varianzanalyse - ANOVA</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Lernziele/Kapitel7_Lernziele.html">Lernziele</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel07/01_Analyse_der_Varianz_ANOVA.html">Varianzanalyse - ANOVA</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Aufgaben/Kapitel07/Aufgaben.html">Übungsaufgaben</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel07/ANOVA_basics.html">Einfaktorielle ANOVA Grundbegriffe</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel07/Einfache_ANOVA.html">Einfaktorielle ANOVA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel07/Bonferroni.html">Bonferroni Korrektur</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lineare Regression</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Lernziele/Kapitel8_Lernziele.html">Lernziele</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel08/01_Lineare_Regression.html">Lineare Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel08/02_Polynomiale_Regression.html">Polynomiale Regression</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Aufgaben/Kapitel08/Aufgaben.html">Übungsaufgaben</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel08/LinReg_basics.html">Lineare Regression - Grundbegriffe</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel08/LineareReg.html">Lineare Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel08/PolyReg.html">Polynomiale Regression</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Logistische Regression</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Lernziele/Kapitel9_Lernziele.html">Lernziele</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Kapitel09/01_Logistische_Regression.html">Logistische Regression</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Aufgaben/Kapitel09/Aufgaben.html">Übungsaufgaben</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel09/LogReg_basics.html">Logistische Regression - Grundbegriffe</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel09/logit_funktion.html">Logistische Funktion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel09/logodds.html">Odds und Log-Odds</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel09/logregmodell.html">Einfaches logistisches Regressionsmodell</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Entscheidungsbäume</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Entscheidungsbäume</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_BaggingBoosting.html">Bagging und Boosting bei Entscheidungsbäumen</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_RandomForest.html">Random Forest</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Aufgaben/Kapitel10/Aufgaben.html">Übungsaufgaben</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel10/decision_trees.html">Entscheidungsbäume</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Aufgaben/Kapitel10/bagging_and_random_forests.html">Bagging &amp; Random Forests</a></li>

</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Literaturverzeichnis</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Literaturverzeichnis.html">Literaturverzeichnis</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Mobile-University-DigitalLab/statistik" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Mobile-University-DigitalLab/statistik/issues/new?title=Issue%20on%20page%20%2FKapitel10/01_DecisionTrees.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Kapitel10/01_DecisionTrees.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Entscheidungsbäume</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regressions-baume">Regressions-Bäume</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regressionsbaume-in-python">Regressionsbäume in Python</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-pruning">Tree Pruning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regressionsbaume-in-python-und-scikit-learn">Regressionsbäume in Python und Scikit-Learn</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#klassifikations-baume">Klassifikations-Bäume</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#klassifikationsbaume-in-python">Klassifikationsbäume in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#klassifikationsbaume-in-python-und-scikit-learn">Klassifikationsbäume in Python und Scikit-Learn</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entscheidungsbaume-vs-lineare-modelle">Entscheidungsbäume vs. Lineare Modelle</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vor-und-nachteile-von-entscheidungsbaumen">Vor- und Nachteile von Entscheidungsbäumen</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="c1"># Load the &quot;autoreload&quot; extension</span>
<span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="c1"># always reload modules</span>
<span class="o">%</span><span class="k">autoreload</span> 2
<span class="c1"># black formatter for jupyter notebooks</span>
<span class="c1"># %load_ext nb_black</span>
<span class="c1"># black formatter for jupyter lab</span>
<span class="o">%</span><span class="k">load_ext</span> lab_black

<span class="o">%</span><span class="k">run</span> ../../src/notebook_env.py
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>---------------------------------
Working on the host: imarevic-pc

---------------------------------
Python version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]

---------------------------------
Python interpreter: /home/imarevic/Documents/teaching/SRH/content/statistik/statistik-env/bin/python3
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="entscheidungsbaume">
<h1>Entscheidungsbäume<a class="headerlink" href="#entscheidungsbaume" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">mean_squared_error</span><span class="p">,</span>
    <span class="n">confusion_matrix</span><span class="p">,</span>
    <span class="n">accuracy_score</span><span class="p">,</span>
    <span class="n">ConfusionMatrixDisplay</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="nn">dtreeviz</span>
</pre></div>
</div>
</div>
</div>
<p>In Kapitel 8 hatten wir die lineare Regression und die polynomiale Regression, als Verfahren zur Vorhersage kontinuierlicher Variablen kennen gelernt. Für die Vorhersage kategorialer Variablen wurde in Kapitel 9 die logistische Regression dargestellt. Im Folgenden werden wir ein Verfahren kennenlernen, das sowohl für Regressions- als auch Klassifikationsprobleme verwenden werden kann, die Entscheidungsbäume.</p>
<p><a href="https://de.wikipedia.org/wiki/Entscheidungsbaum">Entscheidungsbäume</a> sind baumbasierte Verfahren, die sowohl für Regressions-, als auch Klassifikationsprobleme genutzt werden können. Hierbei wir der Vorhersageraum in eine bestimmte Anzahl Regionen <strong>segmentiert</strong> und kann somit als Baumstruktur visualisiert werden (<span id="id1"></span> s.303–314).</p>
<section id="regressions-baume">
<h2>Regressions-Bäume<a class="headerlink" href="#regressions-baume" title="Link to this heading">#</a></h2>
<p>Um Entscheidungsbäume besser zu verstehen, wenden wir uns zunächst dem Fall der Regression zu und behandeln <strong>Regressions-Bäume</strong>. Als Beispiel verwenden wir einen fiktiven Datensatz indem Einkommensdaten gelistet sind. Wir werden versuchen das Einkommen (<code class="docutils literal notranslate"><span class="pre">income</span></code>) anhand der Variablen Anzahl Jahre in der Schule (<code class="docutils literal notranslate"><span class="pre">education</span></code>) und Berufserfahrung (<code class="docutils literal notranslate"><span class="pre">seniority</span></code>) vorherzusagen. Zunächst laden wir den Datensatz:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../../data/income_data.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Eine kurze Visualisierung der Daten zeigt, dass ein Zusammenhang zwischen <code class="docutils literal notranslate"><span class="pre">income</span></code> und den beiden Prädiktoren <code class="docutils literal notranslate"><span class="pre">education</span></code> und <code class="docutils literal notranslate"><span class="pre">seniority</span></code> besteht, wobei der Zusammenhang für <code class="docutils literal notranslate"><span class="pre">education</span></code> etwas stärker ist als für <code class="docutils literal notranslate"><span class="pre">seniority</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">plotting</span><span class="o">.</span><span class="n">scatter_matrix</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/440c32ddc196a7c97c4885cf21dd5cb1862599b212cd1e8a7c029adcb9296ad7.png" src="../_images/440c32ddc196a7c97c4885cf21dd5cb1862599b212cd1e8a7c029adcb9296ad7.png" />
</div>
</div>
<p>Wir greifen nun etwas voraus und fitten einen einfachen Entscheidungsbaum, unter Nutzung der Bibliothel <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, an die Daten und visualisieren den Baum anhand der Splits (hierfür verwenden wir die Bibliothek <code class="docutils literal notranslate"><span class="pre">dtreeviz</span></code>), die für die Variablen <code class="docutils literal notranslate"><span class="pre">education</span></code> und <code class="docutils literal notranslate"><span class="pre">seniority</span></code> im Entscheidungsbaum angewendet wurden. Hier zunächst die Visualisierung:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit a regression tree in order to create</span>
<span class="c1"># the regressor object</span>
<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;education&quot;</span><span class="p">,</span> <span class="s2">&quot;seniority&quot;</span><span class="p">]</span>
<span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;income&quot;</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># create viz model object</span>
<span class="n">viz_model</span> <span class="o">=</span> <span class="n">dtreeviz</span><span class="o">.</span><span class="n">model</span><span class="p">(</span>
    <span class="n">reg</span><span class="p">,</span>
    <span class="n">X_train</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">features</span><span class="p">],</span>
    <span class="n">y_train</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">target</span><span class="p">],</span>
    <span class="n">feature_names</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
    <span class="n">target_name</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># plot view</span>
<span class="n">viz_model</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">fancy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/imarevic/Documents/teaching/SRH/content/statistik/statistik-env/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names
</pre></div>
</div>
<img alt="../_images/502c312f322b9c0c21f6a451f8d7210ad1a748652730a77cd6e53be37887d0fe.svg" src="../_images/502c312f322b9c0c21f6a451f8d7210ad1a748652730a77cd6e53be37887d0fe.svg" />
</div>
</div>
<p>In der erzeugten Visualisierung ist schön zu sehen, dass der Entscheidungsbaum zur Vorhersage des Einkommens bei einer Tiefe von 2 den ersten Split bei 15.59 Jahren in Bezug auf die Anzahl Jahre in der Schule einführt. Eine Ebene darunter wird dann auf Basis der selben Variablen bei 17.86 Jahren, sowie bei einer Seniority von 106.90 gesplittet. Der in jedem “Blatt” des Baumes angezeigte Wert gibt das mittlere Einkommen für die Beobachtungen an, die zu dem jeweiligen Blatt gehören.</p>
<p>Verallgemeinert lässt sich also festhalten, dass in einem Entscheidungsbaum die Ergebnisse eines Splits für Prädiktoren <span class="math notranslate nohighlight">\(X_{j}\)</span> mit <span class="math notranslate nohighlight">\(j = {1, 2, 3, ..., n}\)</span> wie folgt definiert sind:</p>
<div class="math notranslate nohighlight">
\[
Linker Zweig: X_{j} &lt; t_{k}
\]</div>
<div class="math notranslate nohighlight">
\[
Rechter Zweig: X_{j} &gt; t_{k}
\]</div>
<p>wobei <span class="math notranslate nohighlight">\(t_{k}\)</span> den genauen Splitwert bezeichnet.</p>
<p>Würden wir die Daten, bei einer Baumtiefe von 2, in einem Streudiagramm visualisieren, dann liesen sich für jeden der Blätter im Baum eine <strong>Region</strong> einzeichnen. Diese werden anhand der <strong>Splitwerte</strong> bestimmt:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot regions in predictor space</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;mathtext.default&quot;</span><span class="p">:</span> <span class="s2">&quot;regular&quot;</span><span class="p">}</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;education&quot;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;seniority&quot;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">15.59</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">106.9</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mf">0.481</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">17.68</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;$R_</span><span class="si">{1}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-large&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;$R_</span><span class="si">{2}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-large&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">16.5</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">125</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;$R_</span><span class="si">{3}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-large&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">125</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;$R_</span><span class="si">{4}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-large&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Anzahl Schuljahre&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Seniority&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7af58bbe5fe0ac0ccb0e3d1acfaee29206e6be2062f9d289b52082ab61cc3734.png" src="../_images/7af58bbe5fe0ac0ccb0e3d1acfaee29206e6be2062f9d289b52082ab61cc3734.png" />
</div>
</div>
<p>Für jede der vier Regionen können wir den Mittelwert des Einkommens berechnen. Diese sind, wie oben beschrieben, in den Blättern des Baumes angeführt.
Die vier Regionen (auch <strong>Endknoten</strong> oder <strong>Blätter</strong> genannt)  <span class="math notranslate nohighlight">\(R_{1}\)</span> - <span class="math notranslate nohighlight">\(R_{4}\)</span> unterteilen den Prädiktorenraum nach folgenden Eigenschaften:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R_{1} = \{X | education &lt; 15.59, seniority &lt; 106.90\}\)</span> → Personen mit weniger als 15.59 Schuljahren und einer Seniority von höchstens 106.9</p></li>
<li><p><span class="math notranslate nohighlight">\(R_{2} = \{X | education &lt; 15.59, seniority &gt; 106.90\}\)</span> → Personen mit weniger als 15.59 Schuljahren und einer Seniority von mindestens 106.9</p></li>
<li><p><span class="math notranslate nohighlight">\(R_{3} = \{X | 15.59 &lt; education &lt; 17.86 \}\)</span> → Personen zwischen 15.59 und 17.86 Schuljahren</p></li>
<li><p><span class="math notranslate nohighlight">\(R_{4} = \{X | education &gt; 17.86 \}\)</span> → Personen mit mehr als 17.86 Schuljahren</p></li>
</ul>
<p>Anhand der Regionen und der Baumstruktur lassen sich Entscheidungsbäume sehr gut in Worten beschrieben und können somit bei komplexen Datensätzen leichter als klassische Regressionsmdelle interpretiert werden.</p>
<p>Die Hauptelemente eines Entscheidungsbaums lasssen sich also wie folgt zusammenfassen:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Baumelement</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Blatt/Endknoten</p></td>
<td><p>Unterste Ebene des Entscheidungsbaumes nachdem alle Splits vollzogen wurden.</p></td>
</tr>
<tr class="row-odd"><td><p>Interne Knoten</p></td>
<td><p>Knoten, die zwischem dem Ursprung und den Blättern liegen. Entlang dieser Knoten wird gesplittet.</p></td>
</tr>
<tr class="row-even"><td><p>Zweige</p></td>
<td><p>Segmente des Baums, welche die Knoten verbinden.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Beim betrachten eines Entscheidungsbaumes wird schnell deutlich, dass theoretisch bliebig viele Regionen <span class="math notranslate nohighlight">\(R_{n}\)</span> erzeugt werden könnten. Es stellt sich also die Frage wie eine Entscheidungsbaum effizient und möglichst aussagekräftig auf gegebenen Daten erzeugt werden kann. Hierzu sind im Allgemeinen folgende 2 Schritte durchzuführen:</p>
<ol class="arabic simple">
<li><p>Wir teilen den Prädiktorenraum, also die Menge aller möglichen Werte für <span class="math notranslate nohighlight">\(X_{1}, X_{2}, ..., X_{j}\)</span> in <span class="math notranslate nohighlight">\(J\)</span> distinkte und sich nicht überlappende Regionen <span class="math notranslate nohighlight">\(R_{1}, R_{2}, , ..., R_{J}\)</span></p></li>
<li><p>Für jede Beobachtung, die in die Region <span class="math notranslate nohighlight">\(R_{j}\)</span> fällt, machen wir dieselbe Vorhersage anhand des Mittelwerts der Trainingsdaten der entsprechenden Region.</p></li>
</ol>
<p>Wie wählen wir aber die Regionen <span class="math notranslate nohighlight">\(R_{1}, ..., R_{J}\)</span>?</p>
<p>Hierbei ist die Unterteilung in die obigen vier Regionen eine starke Verienfachung. In der Praxis werden die Regionen so gewählt, dass die <strong>Summe der Fehlerquadrate (SSE)</strong> minimiert wird.</p>
<div class="math notranslate nohighlight">
\[SSE = \sum_{j=1}^J\sum_{i \in \mathbb{R}} (y_{i} - \hat y_{R_{j}})^2\]</div>
<p>wobei <span class="math notranslate nohighlight">\(\hat y_{R_{j}}\)</span> den Mittelwert der Trainingsbeobachtungen in ther j-ten Region angibt.</p>
<p>Es wird schnell deutlich, dass es in der Praxis keinen Sinn macht jede mögliche Partitionierung in Regionen durchzuführen um am Ende die beste Konfiguration zu wählen. Daher wird der Entscheidungsbaum in der Praxis mit einem <em>top-down greedy</em> Verfahren aufgebaut. Dieses Verfahren wird auch als <strong>rekursives binäres Splitting</strong> bezeichnet. Hierbei beginnt man oben am Baum und splitted immer in suzzesive 2 Regionen, wobei der Splitpunkt <em>s</em> immer so gewählt wird, dass der Split zur größt möglichen Reduktion im SSE führt. Das Verfahren ist <strong>greedy</strong>, da bei jedem Split des Baumaufbaus immer nur der beste Split zu diesem bestimmten Zeitpunkt definiert wird. Es wird also nicht berücksichtigt ob der Split zum Zeitpunkt <span class="math notranslate nohighlight">\(t_{1}\)</span> ein guter Split für den finalen Baum war, der zu einem späteren Zeitpunkt <span class="math notranslate nohighlight">\(t_{5}\)</span> entstehen könnte. Dieses Verfahren wird so lange wiederholt, bis ein Stopkriterium erfüllt ist (z.B. weniger als 5 Datenpunkte in einer Region).</p>
<p>Der <strong>Top-Down Greedy Entscheidungsbaum Algorithmus</strong> lässt sich also wie folgt formalisieren:</p>
<ol class="arabic">
<li><p>Starte mit dem gesmaten Prädiktorraum und splitte diesen in 2 Regionen unter Selektion des Prädiktors <span class="math notranslate nohighlight">\(X_{j}\)</span> und dem Splitpunkt <span class="math notranslate nohighlight">\(s\)</span>, so dass der Raum in die beiden Regionen <span class="math notranslate nohighlight">\(\{X|X_{j} &lt; s\}\)</span> und <span class="math notranslate nohighlight">\(\{X|X_{j} \geq s\}\)</span> unterteilt wird und zur größt möglichen Reduktion im SSE führt. Die Notation <span class="math notranslate nohighlight">\(\{X|X_{j} &lt; s\}\)</span> bedeutet in diesem Kontext, dass die Region des Prädiktorraums in dem <span class="math notranslate nohighlight">\(X_{j}\)</span> einen Wert kleiner <span class="math notranslate nohighlight">\(s\)</span> annimmt. Das bedeuet, wir berücksichtigen alle Prädiktoren <span class="math notranslate nohighlight">\(X_{1}, ..., X_{p}\)</span> und alle möglichen Werte für den Splitpunkt <span class="math notranslate nohighlight">\(s\)</span> für jeden der Prädiktoren. Formal können wir notieren:</p>
<p>Für jedes <span class="math notranslate nohighlight">\(j\)</span> und <span class="math notranslate nohighlight">\(s\)</span> definieren wir das Paar an Halbebenen</p>
<div class="math notranslate nohighlight">
\[R_{1}(j, s) = \{X|X_{j} &lt; s\}\]</div>
<p>und</p>
<div class="math notranslate nohighlight">
\[R_{2}(j, s) = \{X|X_{j} \geq s\}\]</div>
<p>und wir suchen Werte für <span class="math notranslate nohighlight">\(j\)</span> und <span class="math notranslate nohighlight">\(s\)</span>, welche folgende Gleichung minimieren:</p>
<div class="math notranslate nohighlight">
\[\sum_{i: x_{i} \in R_{1}(j,s)} (y_{i} - \hat y_{R_{1}})^2 + \sum_{i: x_{i} \in R_{2}(j,s)} (y_{i} - \hat y_{R_{2}})^2\]</div>
</li>
<li><p>Wiederhole dieses Vorgehen für den nächsten Split, wobei der SSE nur für diesen Split zum Zeitpunkt <span class="math notranslate nohighlight">\(t_{j}\)</span> minimiert wird.</p></li>
<li><p>Der Prozess endet sobald ein Stoppkriterium <span class="math notranslate nohighlight">\(K\)</span> erreicht ist, wobei <span class="math notranslate nohighlight">\(K\)</span> die Anzahl Datenpunkte in einer Region sein kann. Zum Beispiel <span class="math notranslate nohighlight">\(K=5\)</span>.</p></li>
<li><p>Sobald die Regionen <span class="math notranslate nohighlight">\(R_{1}, ..., R_{J}\)</span> erstellt wurden, wird der Mittelwert zu jeder Beobachtung des Testdatensatzes, die in diese Region fällt, ausgegeben.</p></li>
</ol>
<section id="regressionsbaume-in-python">
<h3>Regressionsbäume in Python<a class="headerlink" href="#regressionsbaume-in-python" title="Link to this heading">#</a></h3>
<p>Lassen Sie uns zur Veranschaulichung den soeben beschriebenen Algorithmus in Python implementieren:</p>
<p>Wir werden den Entscheidungsbaum Regressor als Python Klasse implementieren. Details zu objektorientierter Programmierung sind hier nicht entscheidend und können getrost ignoriert werden. Wichtig ist nur, dass wir die Hauptmethoden der Klasse verstehen. Bevor wir mit der Implementierung starten, benötigen wir ein paar Standardfunktionen, die wir der Klasse hinzufügen:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__init__()</span></code>: wird bei der Instantierung des <code class="docutils literal notranslate"><span class="pre">MyDecisionTreeRegressor()</span></code> Objekts ausgeführt.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">calculate_sse()</span></code> zur Berechnung des SSE</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_overall_sse()</span></code> zur Berechnung des SSE für beide Hälften des Splits. Dieser overall SSE wird mimiert.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">class</span> <span class="nc">MyDecisionTreeRegressor</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_n_leaf_observations</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_n_leaf_observation</span> <span class="o">=</span> <span class="n">max_n_leaf_observation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">calculate_sse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_overall_sse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">left_y</span><span class="p">,</span> <span class="n">right_y</span><span class="p">):</span>
        <span class="n">left_sse</span> <span class="o">=</span> <span class="n">calculate_sse</span><span class="p">(</span><span class="n">left_y</span><span class="p">)</span>
        <span class="n">right_sse</span> <span class="o">=</span> <span class="n">calculate_sse</span><span class="p">(</span><span class="n">right_y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">left_sse</span> <span class="o">+</span> <span class="n">right_sse</span>
</pre></div>
</div>
</div>
</div>
<p>Nun erweitern wir die Implementierung um die <code class="docutils literal notranslate"><span class="pre">fit()</span></code> Methode, welche vom Nutzer verwendet werden kann und die interne Methode <code class="docutils literal notranslate"><span class="pre">fit_tree()</span></code>, in welcher die eigentliche rekursive Implementierung steckt. In dieser Methode ist das Ziel, den besten Split zu bestimmen und dieses Vorgehen rekursiv anzuwenden, bis der beste Split gefunden wurde.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">class</span> <span class="nc">MyDecisionTreeRegressor</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_n_leaf_observations</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_n_leaf_observation</span> <span class="o">=</span> <span class="n">max_n_leaf_observation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">calculate_sse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_overall_sse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">left_y</span><span class="p">,</span> <span class="n">right_y</span><span class="p">):</span>
        <span class="n">left_sse</span> <span class="o">=</span> <span class="n">calculate_sse</span><span class="p">(</span><span class="n">left_y</span><span class="p">)</span>
        <span class="n">right_sse</span> <span class="o">=</span> <span class="n">calculate_sse</span><span class="p">(</span><span class="n">right_y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">left_sse</span> <span class="o">+</span> <span class="n">right_sse</span>

    <span class="k">def</span> <span class="nf">fit_tree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">unique_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1"># Stopkriterien</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unique_y</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_n_leaf_observation</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">unique_y</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
        <span class="k">if</span> <span class="n">depth</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)}</span>

            <span class="c1"># Greedy algorithm um den beste Split zu finden</span>
            <span class="n">best_split</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">best_sse</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">):</span>
                <span class="n">thresholds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">])</span>
                <span class="k">for</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="n">thresholds</span><span class="p">:</span>
                    <span class="n">left_indices</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">threshold</span>
                    <span class="n">right_indices</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span>

                    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">left_indices</span><span class="p">)</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">right_indices</span><span class="p">):</span>
                        <span class="n">left_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">left_indices</span><span class="p">]</span>
                        <span class="n">right_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">right_indices</span><span class="p">]</span>

                        <span class="n">sse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_overall_sse</span><span class="p">(</span><span class="n">left_y</span><span class="p">,</span> <span class="n">right_y</span><span class="p">)</span>

                        <span class="k">if</span> <span class="n">sse</span> <span class="o">&lt;</span> <span class="n">best_sse</span><span class="p">:</span>
                            <span class="n">best_sse</span> <span class="o">=</span> <span class="n">sse</span>
                            <span class="n">best_split</span> <span class="o">=</span> <span class="p">{</span>
                                <span class="s2">&quot;feature&quot;</span><span class="p">:</span> <span class="n">feature</span><span class="p">,</span>
                                <span class="s2">&quot;threshold&quot;</span><span class="p">:</span> <span class="n">threshold</span><span class="p">,</span>
                                <span class="s2">&quot;left&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">left_indices</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">left_indices</span><span class="p">]),</span>
                                <span class="s2">&quot;right&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">right_indices</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">right_indices</span><span class="p">]),</span>
                            <span class="p">}</span>

            <span class="k">if</span> <span class="n">best_split</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)}</span>

            <span class="c1"># rekursiver Aufruf der fir Methode</span>
            <span class="n">left_tree</span> <span class="o">=</span> <span class="n">fit_tree</span><span class="p">(</span><span class="o">*</span><span class="n">best_split</span><span class="p">[</span><span class="s2">&quot;left&quot;</span><span class="p">],</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">right_tree</span> <span class="o">=</span> <span class="n">fit_tree</span><span class="p">(</span><span class="o">*</span><span class="n">best_split</span><span class="p">[</span><span class="s2">&quot;right&quot;</span><span class="p">],</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

            <span class="k">return</span> <span class="p">{</span>
                <span class="s2">&quot;feature&quot;</span><span class="p">:</span> <span class="n">best_split</span><span class="p">[</span><span class="s2">&quot;feature&quot;</span><span class="p">],</span>
                <span class="s2">&quot;threshold&quot;</span><span class="p">:</span> <span class="n">best_split</span><span class="p">[</span><span class="s2">&quot;threshold&quot;</span><span class="p">],</span>
                <span class="s2">&quot;left&quot;</span><span class="p">:</span> <span class="n">left_tree</span><span class="p">,</span>
                <span class="s2">&quot;right&quot;</span><span class="p">:</span> <span class="n">right_tree</span><span class="p">,</span>
            <span class="p">}</span>

        <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
            <span class="c1"># wir updaten das Baum Attribut der Klasse als letzen Schritt</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tree</span> <span class="o">=</span> <span class="n">fit_tree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Folgendes ist bei der Implemetierung der <code class="docutils literal notranslate"><span class="pre">fit()</span></code> Methode zu beachten:</p>
<ul class="simple">
<li><p>Zunächst werden die Stoppkriterien geprüft</p></li>
<li><p>Dann wird über jedes Feature/Prädiktor iterriert und eine <code class="docutils literal notranslate"><span class="pre">threshold</span></code> S so gesucht, dass der SSE minimiert wird.</p></li>
<li><p>Ist dies erfolgt, dann wird der beste Split als <code class="docutils literal notranslate"><span class="pre">best_split</span></code> Dictionary gesetzt</p></li>
<li><p>Zum Schluss, in der eigentlich vom User aufgerufenen Methode, wird das Attribut <code class="docutils literal notranslate"><span class="pre">self.tree</span></code> der Entscheidungsbaum Klasse gesetzt (hier wird dann zur Laufzeit die rekursive Methode <code class="docutils literal notranslate"><span class="pre">fit_tree()</span></code> aufgerufen), sobald der beste Baum mit den besten Splits gefunden wurde. Die Speicherung in <code class="docutils literal notranslate"><span class="pre">self.tree</span></code> ist nützlich, da wir so den gefundenen Baum in der noch zu implementierenden <code class="docutils literal notranslate"><span class="pre">predict()</span></code> Methode verwenden können.</p></li>
</ul>
<p>Lassen Sie uns diese <code class="docutils literal notranslate"><span class="pre">predict()</span></code> Methode als nächstes implementieren. Auch hier splitten wir die Methode wieder in 2 Methoden: Eine, die der User für die gesamten Daten aufruft (<code class="docutils literal notranslate"><span class="pre">predict()</span></code>) und die andere (<code class="docutils literal notranslate"><span class="pre">predict_sample()</span></code>), welche die Implementierung für eine Beobachtung verarbeitet:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">class</span> <span class="nc">MyDecisionTreeRegressor</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_n_leaf_observations</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_n_leaf_observations</span> <span class="o">=</span> <span class="n">max_n_leaf_observations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">calculate_sse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_overall_sse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">left_y</span><span class="p">,</span> <span class="n">right_y</span><span class="p">):</span>
        <span class="n">left_sse</span> <span class="o">=</span> <span class="n">calculate_sse</span><span class="p">(</span><span class="n">left_y</span><span class="p">)</span>
        <span class="n">right_sse</span> <span class="o">=</span> <span class="n">calculate_sse</span><span class="p">(</span><span class="n">right_y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">left_sse</span> <span class="o">+</span> <span class="n">right_sse</span>

    <span class="k">def</span> <span class="nf">fit_tree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">unique_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1"># Stopkriterien</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unique_y</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_n_leaf_observations</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">unique_y</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
        <span class="k">if</span> <span class="n">depth</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)}</span>

            <span class="c1"># Greedy algorithm um den beste Split zu finden</span>
            <span class="n">best_split</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">best_sse</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">):</span>
                <span class="n">thresholds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">])</span>
                <span class="k">for</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="n">thresholds</span><span class="p">:</span>
                    <span class="n">left_indices</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">threshold</span>
                    <span class="n">right_indices</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span>

                    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">left_indices</span><span class="p">)</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">right_indices</span><span class="p">):</span>
                        <span class="n">left_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">left_indices</span><span class="p">]</span>
                        <span class="n">right_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">right_indices</span><span class="p">]</span>

                        <span class="n">sse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_overall_sse</span><span class="p">(</span><span class="n">left_y</span><span class="p">,</span> <span class="n">right_y</span><span class="p">)</span>

                        <span class="k">if</span> <span class="n">sse</span> <span class="o">&lt;</span> <span class="n">best_sse</span><span class="p">:</span>
                            <span class="n">best_sse</span> <span class="o">=</span> <span class="n">sse</span>
                            <span class="n">best_split</span> <span class="o">=</span> <span class="p">{</span>
                                <span class="s2">&quot;feature&quot;</span><span class="p">:</span> <span class="n">feature</span><span class="p">,</span>
                                <span class="s2">&quot;threshold&quot;</span><span class="p">:</span> <span class="n">threshold</span><span class="p">,</span>
                                <span class="s2">&quot;left&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">left_indices</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">left_indices</span><span class="p">]),</span>
                                <span class="s2">&quot;right&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">right_indices</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">right_indices</span><span class="p">]),</span>
                            <span class="p">}</span>

            <span class="k">if</span> <span class="n">best_split</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)}</span>

            <span class="c1"># rekursiver Aufruf der fir Methode</span>
            <span class="n">left_tree</span> <span class="o">=</span> <span class="n">fit_tree</span><span class="p">(</span><span class="o">*</span><span class="n">best_split</span><span class="p">[</span><span class="s2">&quot;left&quot;</span><span class="p">],</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">right_tree</span> <span class="o">=</span> <span class="n">fit_tree</span><span class="p">(</span><span class="o">*</span><span class="n">best_split</span><span class="p">[</span><span class="s2">&quot;right&quot;</span><span class="p">],</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

            <span class="k">return</span> <span class="p">{</span>
                <span class="s2">&quot;feature&quot;</span><span class="p">:</span> <span class="n">best_split</span><span class="p">[</span><span class="s2">&quot;feature&quot;</span><span class="p">],</span>
                <span class="s2">&quot;threshold&quot;</span><span class="p">:</span> <span class="n">best_split</span><span class="p">[</span><span class="s2">&quot;threshold&quot;</span><span class="p">],</span>
                <span class="s2">&quot;left&quot;</span><span class="p">:</span> <span class="n">left_tree</span><span class="p">,</span>
                <span class="s2">&quot;right&quot;</span><span class="p">:</span> <span class="n">right_tree</span><span class="p">,</span>
            <span class="p">}</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># wir updaten das Baum Attribut der Klasse als letzen Schritt</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_tree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tree</span><span class="p">):</span>
        <span class="k">if</span> <span class="s2">&quot;value&quot;</span> <span class="ow">in</span> <span class="n">tree</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tree</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="n">tree</span><span class="p">[</span><span class="s2">&quot;feature&quot;</span><span class="p">]]</span> <span class="o">&lt;=</span> <span class="n">tree</span><span class="p">[</span><span class="s2">&quot;threshold&quot;</span><span class="p">]:</span>
            <span class="k">return</span> <span class="n">predict_sample</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tree</span><span class="p">[</span><span class="s2">&quot;left&quot;</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">predict_sample</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tree</span><span class="p">[</span><span class="s2">&quot;right&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_sample</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Es wird deutlich, dass wir den in der <code class="docutils literal notranslate"><span class="pre">fit()</span></code> Methode erzeugten Entscheidungsbaum hier nun nutzen und die darin gespeicherten Splitwerte für jeden Datenpunkt anwenden. Ebenfalls hervorzuheben ist, dass zu Beginn der <code class="docutils literal notranslate"><span class="pre">predict_sample()</span></code> Methode zunächst geprüft wird ob <code class="docutils literal notranslate"><span class="pre">&quot;value&quot;</span> <span class="pre">in</span> <span class="pre">tree</span></code>. Diese einfache Prüfung geht auf das Setzen des Strings “value” als der Default für die Stopkriterien in der <code class="docutils literal notranslate"><span class="pre">fit()</span></code> Methode zurück.</p>
<p>Hier ein Beispiellauf des soeben implementierten Models:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Beispieldaten</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c1"># Instantierung des Models nud Modelfitting</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyDecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_n_leaf_observations</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Nach Modelfit rufen wir die Vorhersagemethode auf</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">]])</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predictions:&quot;</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predictions: [2.2 2.2]
</pre></div>
</div>
</div>
</div>
<p>Anhand dieser Implementiernug lassen sich Entscheidungsbäume gut nachvollziehen. Bitte beachten Sie jedoch, dass dies <strong>nur ein Beispiel zur Veranschaulichung</strong> ist und das Ergebnis von professionellen Implementierungen wie in der <code class="docutils literal notranslate"><span class="pre">scikit-klearn</span></code> Bibliothek abweichen kann. Gründe hierfür sind unter anderen folgende:</p>
<ul class="simple">
<li><p>sklearn implementiert Entscheidungsbäume effizient und nutzt bessere Algorithmen als unser naives Beispiel (siehe Tree Pruning im nächsten Abschnitt)</p></li>
<li><p>sklearn nutzt dem MSE anstatt des SSE.</p></li>
</ul>
<p>Nichts desto trotz können wir aus obigem Beispiel die Grundlagen des Greedy Entscheidungsbaum Algorithmus verstehen.
Im nächsten Abschnitt werden wir auf eine mögliche Erweiterung hinweisen (Tree Pruning), die in professionellen Implementierungen häufig Anwendung findet und zu weit aus besseren Ergebnissen führt als die einfache Greedy Entscheidungsbaum Variante.</p>
</section>
</section>
<section id="tree-pruning">
<h2>Tree Pruning<a class="headerlink" href="#tree-pruning" title="Link to this heading">#</a></h2>
<p>Das soeben beschriebene Vorgehen kann sehr gute Vorhersagen auf dem Trainigsdatensatz liefern, kann aber dazu neigen auf zuvor ungesehenen Validierungs- oder Testdatensätzen schlechte Vorhersagen zu liefern. Dies liegt daran, dass der greedy Ansatz dazu neigt möglichst goße Entscheidugsbäume aufzuspannen und somit die Trainingsadten zu <strong>overfitten</strong>. Ein kleinerer Baum, der weniger Spits und somit weniger Regionen <span class="math notranslate nohighlight">\(R_{1} ... R_{n}\)</span> enthällt, kann aufgrund geringerer Varianz zu besseren generalisierten Vorhersagen auf den Testdaten führen.</p>
<p>Eine Lösung des Problems könnte sein, weniger Splits beim Aufbau des Entscheidungsbaumes zuzulassen, indem man eine Grenze für die relative Verbesserung des SSE vorgibt und somit ab einer bestimmten marginalen Verbessernug des SEE keine weiteren Splits zulässt. Nachteil dieses Verfahrens ist jedoch, dass eventuell “bedeutungslose” Splits zu Beginn des Aufbaus auftreten könnten, wodurch eine spätere Verbesserung des SSE, durch einen weiteren Split, der theoretisch gut wäre, aber aufgrund des Abbruchkriteriums nicht mehr zustande kommt, verhindert wird.
Daher zieht man es in der Praxis vor, einen sehr <strong>tiefen Entscheidungsbaum</strong> zu generieren und diesen dann zu <strong>trimmen</strong> (zurück schneiden) um einen <strong>Sub-Baum</strong> zu erhalten.</p>
<p>Das hierfür verwendete Verfahren wird in der Literatur auch <strong>Cost-complexity pruning</strong> oder <strong>weakest link pruning</strong> genannt und kann wie folgt beschrieben werden:</p>
<ol class="arabic simple">
<li><p>Baue eine Entscheidungsbaum durch einaches binäres Splitten auf, bis ein Stoppkriterium (z.B. N&lt;5) erreicht ist (siehe <strong>Top-Down Greedy Entscheidungsbaum Algorithmus</strong>). Dieser tiefe Baum wird als <span class="math notranslate nohighlight">\(T_{0}\)</span> bezeichnet.</p></li>
<li><p>Wende <strong>Cost-complexity pruning</strong> auf den tiefen Entscheidungsbaum an um eine Sequenz an Subbäumen <span class="math notranslate nohighlight">\(T\)</span> in Abhängigkeit eines Skalierungsparameters <span class="math notranslate nohighlight">\(\alpha\)</span> zu erhalten. Zu jedem <span class="math notranslate nohighlight">\(\alpha\)</span> gehört also ein Subbaum <span class="math notranslate nohighlight">\(T\)</span>, sodass gilt:</p></li>
</ol>
<p>$<span class="math notranslate nohighlight">\(\sum_{m=1}^{\mid T\mid} \sum_{i: x_{i} \in R_{m}} (y_{i} - \hat y_{R_{m}})^2 + \alpha \mid T \mid\)</span>$,</p>
<p>wobei <span class="math notranslate nohighlight">\(\mid T \mid\)</span> das Minimum der Anzahl Blätter von <span class="math notranslate nohighlight">\(T\)</span> darstellt, <span class="math notranslate nohighlight">\(R_{m}\)</span> die Region des Prädiktorraums, die zum <span class="math notranslate nohighlight">\(m\)</span>-ten Blatt gehört, und <span class="math notranslate nohighlight">\(y_{R_{m}})\)</span> stellt den vorhergesagten Wert dar, der zur Region <span class="math notranslate nohighlight">\(R_{m}\)</span> gehört, also der Mittelwert der Trainingsdaten in dieser Region.</p>
<p>Daraus folgt:</p>
<p>Wenn <span class="math notranslate nohighlight">\(\alpha = 0\)</span>, dann ist Subbaum <span class="math notranslate nohighlight">\(T = T_{0}\)</span>.</p>
<p>Wenn <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>, dann wird <span class="math notranslate nohighlight">\(\mid T \mid\)</span> kleiner und folglich auch der Subbaum <span class="math notranslate nohighlight">\(T\)</span>. Dies erlaubt ein kontrolliertes <strong>Trimmen</strong> (engl. pruning) des Baumes in Abhängigkeit von <span class="math notranslate nohighlight">\(\alpha\)</span> unter Nutzung von Validierungsdaten.</p>
<ol class="arabic simple" start="3">
<li><p>Nutze Krossvalidierung um <span class="math notranslate nohighlight">\(\alpha\)</span> zu wählen, sodass der mittlere Vorhersagefehler über alle Validierungssätze minimal ist.</p></li>
<li><p>Kehre zum Subbaum aus Schritt 2 zurück, der zu dem gewählten <span class="math notranslate nohighlight">\(\alpha\)</span> gehört.</p></li>
</ol>
<section id="regressionsbaume-in-python-und-scikit-learn">
<h3>Regressionsbäume in Python und Scikit-Learn<a class="headerlink" href="#regressionsbaume-in-python-und-scikit-learn" title="Link to this heading">#</a></h3>
<p>Im folgenden können wir so einen Entscheidungbaum auf den <strong>Hitters-Datenatz</strong> anwenden. Dieser Datensatz enthällt Features von Baseball Spielern anhand derer das Gehalt der Spieler vorhergesagt werden soll. Wir verwenden bei der Implementierung <code class="docutils literal notranslate"><span class="pre">sckit-learn</span></code>Bibliothek. Das Beispiel soll zeigen, wie der Testfehler nach einer bestimmten Anzahl an Blättern nicht mehr besser wird:</p>
<p>Laden wir zunächst die Daten:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../../data/hitters.csv&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AtBat</th>
      <th>Hits</th>
      <th>HmRun</th>
      <th>Runs</th>
      <th>RBI</th>
      <th>Walks</th>
      <th>Years</th>
      <th>CAtBat</th>
      <th>CHits</th>
      <th>CHmRun</th>
      <th>CRuns</th>
      <th>CRBI</th>
      <th>CWalks</th>
      <th>League</th>
      <th>Division</th>
      <th>PutOuts</th>
      <th>Assists</th>
      <th>Errors</th>
      <th>Salary</th>
      <th>NewLeague</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>315</td>
      <td>81</td>
      <td>7</td>
      <td>24</td>
      <td>38</td>
      <td>39</td>
      <td>14</td>
      <td>3449</td>
      <td>835</td>
      <td>69</td>
      <td>321</td>
      <td>414</td>
      <td>375</td>
      <td>N</td>
      <td>W</td>
      <td>632</td>
      <td>43</td>
      <td>10</td>
      <td>475.0</td>
      <td>N</td>
    </tr>
    <tr>
      <th>2</th>
      <td>479</td>
      <td>130</td>
      <td>18</td>
      <td>66</td>
      <td>72</td>
      <td>76</td>
      <td>3</td>
      <td>1624</td>
      <td>457</td>
      <td>63</td>
      <td>224</td>
      <td>266</td>
      <td>263</td>
      <td>A</td>
      <td>W</td>
      <td>880</td>
      <td>82</td>
      <td>14</td>
      <td>480.0</td>
      <td>A</td>
    </tr>
    <tr>
      <th>3</th>
      <td>496</td>
      <td>141</td>
      <td>20</td>
      <td>65</td>
      <td>78</td>
      <td>37</td>
      <td>11</td>
      <td>5628</td>
      <td>1575</td>
      <td>225</td>
      <td>828</td>
      <td>838</td>
      <td>354</td>
      <td>N</td>
      <td>E</td>
      <td>200</td>
      <td>11</td>
      <td>3</td>
      <td>500.0</td>
      <td>N</td>
    </tr>
    <tr>
      <th>4</th>
      <td>321</td>
      <td>87</td>
      <td>10</td>
      <td>39</td>
      <td>42</td>
      <td>30</td>
      <td>2</td>
      <td>396</td>
      <td>101</td>
      <td>12</td>
      <td>48</td>
      <td>46</td>
      <td>33</td>
      <td>N</td>
      <td>E</td>
      <td>805</td>
      <td>40</td>
      <td>4</td>
      <td>91.5</td>
      <td>N</td>
    </tr>
    <tr>
      <th>5</th>
      <td>594</td>
      <td>169</td>
      <td>4</td>
      <td>74</td>
      <td>51</td>
      <td>35</td>
      <td>11</td>
      <td>4408</td>
      <td>1133</td>
      <td>19</td>
      <td>501</td>
      <td>336</td>
      <td>194</td>
      <td>A</td>
      <td>W</td>
      <td>282</td>
      <td>421</td>
      <td>25</td>
      <td>750.0</td>
      <td>A</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Als nächstes dummy-enkodieren wir die kategorialen Variablen und standartisieren alle metrischen Prädiktoren:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Enkodierung der Daten</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">#  Features und Target definieren</span>
<span class="n">X_df</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;Salary&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_df</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;Salary&quot;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X_df</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y_df</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># Standardtisierung der Daten</span>
<span class="n">scaler_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_standardized</span> <span class="o">=</span> <span class="n">scaler_X</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">scaler_y</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">y_standardized</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Im letzten Schritt können wir nun die Daten in <strong>Trainings-, Validierungs-, und Testdaten</strong> aufteilen und den Entscheidungsbaum trainieren.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Splitten der Daten in train, validatio und test set</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_temp</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_temp</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_standardized</span><span class="p">,</span> <span class="n">y_standardized</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
<span class="n">X_valid</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_temp</span><span class="p">,</span> <span class="n">y_temp</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># Leere Listen zur Speicherung der Ergebnisse</span>
<span class="n">train_mse</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">valid_mse</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_mse</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Fitten der &quot;pruned&quot; Entscheidungsbäume</span>
<span class="k">for</span> <span class="n">n_nodes</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">):</span>
    <span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">y_valid_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span>
    <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

    <span class="n">train_mse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">))</span>
    <span class="n">valid_mse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_valid</span><span class="p">,</span> <span class="n">y_valid_pred</span><span class="p">))</span>
    <span class="n">test_mse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">))</span>

<span class="c1"># Erzeuge Plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">train_mse</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train MSE&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">valid_mse</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation MSE&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">test_mse</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Test MSE&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Anzahl Knoten&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Squared Error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;MSE als Funktion der Anzahl finaler Knoten des pruned Entscheidungsbaumes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f23e89fed008e450641a1d4b92eba7d78b1601d137504ea1fdb687c5e936e8c0.png" src="../_images/f23e89fed008e450641a1d4b92eba7d78b1601d137504ea1fdb687c5e936e8c0.png" />
</div>
</div>
<p>In dem obigen Plot sehen wir, dass <strong>Pruning</strong> durchaus Sinn macht, da zu tiefe Bäume zu Overfitting neigen: Der Entscheidungsbaum beschreibt die Trainingsdaten sehr gut, generalisiert aber nicht auf das Validierungs- und Test-Set der Daten. Nach 4-5 Knoten im Baum reduziert sich zwar der Fehler (in unserem Beispiel der Mean-Squared-Error (MSE)) auf den Trainingsdaten, jedoch nicht auf den Testdaten.</p>
</section>
</section>
<section id="klassifikations-baume">
<h2>Klassifikations-Bäume<a class="headerlink" href="#klassifikations-baume" title="Link to this heading">#</a></h2>
<p><strong>Klassifikationsbäume</strong> sind Regressionsbäumen sehr ähnlich, mit dem Unterschied, das hier eine kategoriale Variable vorhergsagt wird und nicht eine intervall skalierte oder metrische Variable. Das bedeutet, dass bei der Vorhersage nicht der Mittelwert, der zu der Region und assozierten Blättern des Entscheidungsbaumes gehörenden Beobachtungen zurück geliefert wird, sondern die am <strong>häufigsten auftretende Klasse in den Trainingsdaten</strong> der jeweiligen Region.</p>
<p>Es wird somit auch schnell deutlich, dass wir für Klassifikationsbäume den SSE nicht mehr verwenden können. Anstelle des SSE kommt hier die <strong>Klassifikations-Fehlerrate</strong> zum Einsatz. Diese defniert sich als der Anteil an Trainingsdaten in einer bestimmten Region <span class="math notranslate nohighlight">\(R_{n}\)</span>, die nicht zur häufigsten Klasse dieser Region gehören.</p>
<p>Formal können wir dies wie folgt schreiben:</p>
<div class="math notranslate nohighlight">
\[
E = 1 - \max_{k}(\hat p_{mk})
\]</div>
<p><span class="math notranslate nohighlight">\(\hat p_{mk}\)</span> steht in dieser Gleichung also für den Anteil der Trainingdaten oder Traininsgbeobachtungen, die der m-ten Region der k-ten Klasse angehören.</p>
<p>In der Praxis hat sich herausgestellt, das dieser Klassifikationsfehler nicht sensitiv genug ist um Klassifikationsbäume aufzubauen. Daher kommen in der Praxis zwei bessere Maße zum Einsatz:</p>
<ul class="simple">
<li><p>der <strong>Gini Index</strong></p></li>
<li><p>das Maß der <strong>Cross-Entropy</strong></p></li>
</ul>
<p>Der <a href="https://en.wikipedia.org/wiki/Gini_coefficient">Gini-Index</a> ist wie folgt definiert:</p>
<div class="math notranslate nohighlight">
\[
G = \sum_{k=1}^K \hat p_{mk} (1-\hat p_{mk})
\]</div>
<p>Eine Alternative zum Gini Index ist die <a href="https://en.wikipedia.org/wiki/Cross-entropy">Cross-Entropy</a>. Sie ist wie folgt definiert:</p>
<div class="math notranslate nohighlight">
\[
D = - \sum_{k=1}^K \hat p_{mk} \log{p_{mk}},
\]</div>
<p>da <span class="math notranslate nohighlight">\(0 \leq \hat p_{mk} \leq 1\)</span>, folgt dass <span class="math notranslate nohighlight">\(0 \leq -\hat p_{mk} \log{p_{mk}}\)</span>. Die Cross-Entropy D nimmt also nahe null an wenn die <span class="math notranslate nohighlight">\(\hat p_{mk}\)</span> der Entscheidungsbäume nahe 0 oder exakt 0 sind. Daher nimmt die Cross-Entropy, analog dem Gini-Index kleinere Werte annimmt, wenn der m-te Knoten des Baumes “pur” ist, also größtenteils Beobachtugen nur einer Klasse enthällt.</p>
<section id="klassifikationsbaume-in-python">
<h3>Klassifikationsbäume in Python<a class="headerlink" href="#klassifikationsbaume-in-python" title="Link to this heading">#</a></h3>
<p>Lassen Sie uns analog den Regressions-Bäumen auch einen einfachen Klassifikatoinsbaum in Python implementieren.</p>
<p>Zunächste benötigen wir die soeben erläuterten Funktionen zur Berechnung des Gini-Index (wir werden diese der Einfachhalt halber verwenden), sowie ein paar Hilfsfunktioen zur Bestimmung der Anzahl Labels. Folgende Methoden fügen wir unserer Klasse <code class="docutils literal notranslate"><span class="pre">MyDecisionTreeClassifier</span></code> zunächst hinzu:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__init__()</span></code>: wird bei der Instantierung des <code class="docutils literal notranslate"><span class="pre">MyDecisionTreeClassifier()</span></code> Objekts ausgeführt.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">calculate_gini()</span></code> und <code class="docutils literal notranslate"><span class="pre">gini_impurity()</span></code>: berechnen den Gini-Index und die “purheit” an einem bestimmten Knoten.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">count_labels()</span></code> und <code class="docutils literal notranslate"><span class="pre">most_common_label()</span></code>: zählen Labels und geben das häufigste Label zurück.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyDecisionTreeClassifier</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">calculate_gini</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">left_y</span><span class="p">,</span> <span class="n">right_y</span><span class="p">):</span>
        <span class="n">left_impurity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gini_impurity</span><span class="p">(</span><span class="n">left_y</span><span class="p">)</span>
        <span class="n">right_impurity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gini_impurity</span><span class="p">(</span><span class="n">right_y</span><span class="p">)</span>
        <span class="n">total_impurity</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">left_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">left_impurity</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">right_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">right_impurity</span>
        <span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">left_y</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">right_y</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">total_impurity</span>

    <span class="k">def</span> <span class="nf">gini_impurity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_labels</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">impurity</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">((</span><span class="n">count</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">counts</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">impurity</span>

    <span class="k">def</span> <span class="nf">count_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">y</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">counts</span><span class="p">:</span>
                <span class="n">counts</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">counts</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">counts</span>

    <span class="k">def</span> <span class="nf">most_common_label</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_labels</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">most_common</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">counts</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">most_common</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Nun erweitern wir die Implementierung um die Methode <code class="docutils literal notranslate"><span class="pre">grow_tree()</span></code>, welche den Baum aufspannt, sowie die Methode <code class="docutils literal notranslate"><span class="pre">best_split()</span></code>, die den besten Split bestimmt und diesen zusammen mit den besten Features zurück gibt. Die <code class="docutils literal notranslate"><span class="pre">grow_tree()</span></code> Methode wird final in der <code class="docutils literal notranslate"><span class="pre">fit()</span></code> Methode aufgerufen, welche auf vom Endnutzer aufgerufen wird:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyDecisionTreeClassifier</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grow_tree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">grow_tree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
        <span class="c1"># Prüfe ob alle Label identisch sind</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">depth</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">most_common_label</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1"># Finde den besten Split</span>
        <span class="n">best_feature</span><span class="p">,</span> <span class="n">best_threshold</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">best_feature</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">most_common_label</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">left_indices</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">best_feature</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">best_threshold</span>
        <span class="n">right_indices</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">best_feature</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">best_threshold</span>

        <span class="n">left_subtree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grow_tree</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">left_indices</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">left_indices</span><span class="p">],</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">right_subtree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grow_tree</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">right_indices</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">right_indices</span><span class="p">],</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">best_feature</span><span class="p">,</span> <span class="n">best_threshold</span><span class="p">,</span> <span class="n">left_subtree</span><span class="p">,</span> <span class="n">right_subtree</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">best_split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">best_gini</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
        <span class="n">best_feature</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">best_threshold</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">):</span>
            <span class="n">thresholds</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="n">thresholds</span><span class="p">:</span>
                <span class="n">left_indices</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">threshold</span>
                <span class="n">right_indices</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">left_indices</span><span class="p">]))</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">right_indices</span><span class="p">]))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">gini</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_gini</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">left_indices</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">right_indices</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">gini</span> <span class="o">&lt;</span> <span class="n">best_gini</span><span class="p">:</span>
                    <span class="n">best_gini</span> <span class="o">=</span> <span class="n">gini</span>
                    <span class="n">best_feature</span> <span class="o">=</span> <span class="n">feature</span>
                    <span class="n">best_threshold</span> <span class="o">=</span> <span class="n">threshold</span>

        <span class="k">return</span> <span class="n">best_feature</span><span class="p">,</span> <span class="n">best_threshold</span>

    <span class="k">def</span> <span class="nf">calculate_gini</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">left_y</span><span class="p">,</span> <span class="n">right_y</span><span class="p">):</span>
        <span class="n">left_impurity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gini_impurity</span><span class="p">(</span><span class="n">left_y</span><span class="p">)</span>
        <span class="n">right_impurity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gini_impurity</span><span class="p">(</span><span class="n">right_y</span><span class="p">)</span>
        <span class="n">total_impurity</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">left_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">left_impurity</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">right_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">right_impurity</span>
        <span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">left_y</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">right_y</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">total_impurity</span>

    <span class="k">def</span> <span class="nf">gini_impurity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_labels</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">impurity</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">((</span><span class="n">count</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">counts</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">impurity</span>

    <span class="k">def</span> <span class="nf">count_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">y</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">counts</span><span class="p">:</span>
                <span class="n">counts</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">counts</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">counts</span>

    <span class="k">def</span> <span class="nf">most_common_label</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_labels</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">most_common</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">counts</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">most_common</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Zuletzt fügen wir noch die vom Nutzer verwendete Methode <code class="docutils literal notranslate"><span class="pre">predict()</span></code> hinzu, welche intern die Methode <code class="docutils literal notranslate"><span class="pre">predict_sample()</span></code> verwendet:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyDecisionTreeClassifier</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grow_tree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">grow_tree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
        <span class="c1"># Prüfe ob alle Label identisch sind</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">depth</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">most_common_label</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1"># Finde den besten Split</span>
        <span class="n">best_feature</span><span class="p">,</span> <span class="n">best_threshold</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">best_feature</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">most_common_label</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">left_indices</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">best_feature</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">best_threshold</span>
        <span class="n">right_indices</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">best_feature</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">best_threshold</span>

        <span class="n">left_subtree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grow_tree</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">left_indices</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">left_indices</span><span class="p">],</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">right_subtree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grow_tree</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">right_indices</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">right_indices</span><span class="p">],</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">best_feature</span><span class="p">,</span> <span class="n">best_threshold</span><span class="p">,</span> <span class="n">left_subtree</span><span class="p">,</span> <span class="n">right_subtree</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">best_split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">best_gini</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
        <span class="n">best_feature</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">best_threshold</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">):</span>
            <span class="n">thresholds</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="n">thresholds</span><span class="p">:</span>
                <span class="n">left_indices</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">threshold</span>
                <span class="n">right_indices</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">left_indices</span><span class="p">]))</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">right_indices</span><span class="p">]))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">gini</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_gini</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">left_indices</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">right_indices</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">gini</span> <span class="o">&lt;</span> <span class="n">best_gini</span><span class="p">:</span>
                    <span class="n">best_gini</span> <span class="o">=</span> <span class="n">gini</span>
                    <span class="n">best_feature</span> <span class="o">=</span> <span class="n">feature</span>
                    <span class="n">best_threshold</span> <span class="o">=</span> <span class="n">threshold</span>

        <span class="k">return</span> <span class="n">best_feature</span><span class="p">,</span> <span class="n">best_threshold</span>

    <span class="k">def</span> <span class="nf">calculate_gini</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">left_y</span><span class="p">,</span> <span class="n">right_y</span><span class="p">):</span>
        <span class="n">left_impurity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gini_impurity</span><span class="p">(</span><span class="n">left_y</span><span class="p">)</span>
        <span class="n">right_impurity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gini_impurity</span><span class="p">(</span><span class="n">right_y</span><span class="p">)</span>
        <span class="n">total_impurity</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">left_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">left_impurity</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">right_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">right_impurity</span>
        <span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">left_y</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">right_y</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">total_impurity</span>

    <span class="k">def</span> <span class="nf">gini_impurity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_labels</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">impurity</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">((</span><span class="n">count</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">counts</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">impurity</span>

    <span class="k">def</span> <span class="nf">count_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">y</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">counts</span><span class="p">:</span>
                <span class="n">counts</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">counts</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">counts</span>

    <span class="k">def</span> <span class="nf">most_common_label</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_labels</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">most_common</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">counts</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">most_common</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">feature</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">left_subtree</span><span class="p">,</span> <span class="n">right_subtree</span> <span class="o">=</span> <span class="n">node</span>
            <span class="k">if</span> <span class="n">sample</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">left_subtree</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">right_subtree</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">node</span>
</pre></div>
</div>
</div>
</div>
<p>Folgende Punkte sind zur Implementierung hervorzuheben:</p>
<ul class="simple">
<li><p>die Methode <code class="docutils literal notranslate"><span class="pre">grow_tree()</span></code> wird rekursiv aufgerufen, sodass sobald ein Split generiert wurde an diesem Knoten erneut ein Entscheidungsbaum der Tiefe 1 generiert wird. Daraufhin wird beim nächsten rekursiven Aufruf die Tiefe <code class="docutils literal notranslate"><span class="pre">depth</span></code> um 1 inkrementiert. Dies geschieht so lange bis ein Abbruchkriterium erfüllt ist und es wird das Tuple <code class="docutils literal notranslate"><span class="pre">(best_feature,</span> <span class="pre">best_threshold,</span> <span class="pre">left_subtree,</span> <span class="pre">right_subtree)</span></code> zurückgeliefert.</p></li>
<li><p>die Methode <code class="docutils literal notranslate"><span class="pre">best_split()</span></code> nutzt den Gini-Index um den besten Splitpunkt zu liefern. Der einfachhalt halber wird hier auch das beste feature ebenso zurück geliefert: <code class="docutils literal notranslate"><span class="pre">best_feature,</span> <span class="pre">best_threshold</span></code>.</p></li>
<li><p>in der Methode <code class="docutils literal notranslate"><span class="pre">fit()</span></code> ist zu sehen, wie das Attribut <code class="docutils literal notranslate"><span class="pre">self.tree</span></code> der Klasse gesetzt wird, sobald der Klassifikationsbaum vollständig rekursiv generiert wurde. Dieses Attribut wird beim Aufruf von <code class="docutils literal notranslate"><span class="pre">predict()</span></code> dann später verwendet.</p></li>
</ul>
<p>Auch bei dieser Implementierung ist hervorzuheben, dass es sich nur um ein <strong>anschauliches Beispiel</strong> handelt, anhend dessen wir Klassifikationsbäume besser verstehen können.</p>
<p>Hier ein Beispiellauf des eben implementierten Models:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Beispiel</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># Instantierung des Klassifikators</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">MyDecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Vorhersage auf Test-Daten</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predictions: </span><span class="si">{</span><span class="n">predictions</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predictions: [1 1 1 1 1]
</pre></div>
</div>
</div>
</div>
<section id="klassifikationsbaume-in-python-und-scikit-learn">
<h4>Klassifikationsbäume in Python und Scikit-Learn<a class="headerlink" href="#klassifikationsbaume-in-python-und-scikit-learn" title="Link to this heading">#</a></h4>
<p>Unter Verwendung der Bibliothek <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> können wir auch für Klassifikationsbäume eine Anwendungsbeispiel betrachten. Wir nehmen hierzu den <strong>Hurricanes</strong> Datensatz, der Ihnen schon aus dem Kapitel zur logistischen Regression bekannt ist, und wenden den in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> zur Verfügung stehenden <code class="docutils literal notranslate"><span class="pre">DecissionTreeClassifier()</span></code> an. Lassen Sie uns zunächst die Daten laden:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hurricanes</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s2">&quot;../../data/hurricanes.xlsx&quot;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">hurricanes</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Number</th>
      <th>Name</th>
      <th>Year</th>
      <th>Type</th>
      <th>FirstLat</th>
      <th>FirstLon</th>
      <th>MaxLat</th>
      <th>MaxLon</th>
      <th>LastLat</th>
      <th>LastLon</th>
      <th>MaxInt</th>
    </tr>
    <tr>
      <th>RowNames</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>430</td>
      <td>NOTNAMED</td>
      <td>1944</td>
      <td>1</td>
      <td>30.2</td>
      <td>-76.1</td>
      <td>32.1</td>
      <td>-74.8</td>
      <td>35.1</td>
      <td>-69.2</td>
      <td>80</td>
    </tr>
    <tr>
      <th>2</th>
      <td>432</td>
      <td>NOTNAMED</td>
      <td>1944</td>
      <td>0</td>
      <td>25.6</td>
      <td>-74.9</td>
      <td>31.0</td>
      <td>-78.1</td>
      <td>32.6</td>
      <td>-78.2</td>
      <td>80</td>
    </tr>
    <tr>
      <th>3</th>
      <td>433</td>
      <td>NOTNAMED</td>
      <td>1944</td>
      <td>0</td>
      <td>14.2</td>
      <td>-65.2</td>
      <td>16.6</td>
      <td>-72.2</td>
      <td>20.6</td>
      <td>-88.5</td>
      <td>105</td>
    </tr>
    <tr>
      <th>4</th>
      <td>436</td>
      <td>NOTNAMED</td>
      <td>1944</td>
      <td>0</td>
      <td>20.8</td>
      <td>-58.0</td>
      <td>26.3</td>
      <td>-72.3</td>
      <td>42.1</td>
      <td>-71.5</td>
      <td>120</td>
    </tr>
    <tr>
      <th>5</th>
      <td>437</td>
      <td>NOTNAMED</td>
      <td>1944</td>
      <td>0</td>
      <td>20.0</td>
      <td>-84.2</td>
      <td>20.6</td>
      <td>-84.9</td>
      <td>19.1</td>
      <td>-93.9</td>
      <td>70</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Wir werden versuchen die binäre Antwortvariable <code class="docutils literal notranslate"><span class="pre">Origins</span></code> anhand der Prädiktorvariablen <code class="docutils literal notranslate"><span class="pre">FirstLat</span></code> vorherzusagen.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Benamung der Origin</span>
<span class="n">hurricanes</span><span class="p">[</span><span class="s2">&quot;Origins&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">hurricanes</span><span class="p">[</span><span class="s2">&quot;Type&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
    <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;tropisch&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;aussertropisch&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="s2">&quot;aussertropisch&quot;</span><span class="p">}</span>
<span class="p">)</span>
<span class="c1"># Erstellung von X und y Datensätzen</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">hurricanes</span><span class="p">[</span><span class="s2">&quot;FirstLat&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">hurricanes</span><span class="p">[</span><span class="s2">&quot;Origins&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="s2">&quot;tropisch&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;aussertropisch&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_21123/1118630555.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option(&#39;future.no_silent_downcasting&#39;, True)`
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># test train split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Fitten das Klassifikators an die Trainingsdaten</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;DecisionTreeClassifier<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeClassifier.html">?<span>Documentation for DecisionTreeClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)</pre></div> </div></div></div></div></div></div>
</div>
<p>Nachdem das Modell an die Trainingsdaten gefitted wurde, können wir durch Aufruf der <code class="docutils literal notranslate"><span class="pre">predict()</span></code> Methode Vorhersagen für die Klasse erhalten.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
       1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
       1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0])
</pre></div>
</div>
</div>
</div>
<p>Abschliessend können wir, die vom Modell vorhergesagten Werte, mit den echten Werten im Testdatensatz vergleich um die Performanz des Modells zu evaluieren. Dies können wir anhand der <strong>Accuracy</strong> des Models machen.</p>
<p>Diese ist definert als</p>
<div class="math notranslate nohighlight">
\[
\text{Accuracy} =
\frac{\text{korrekte Klassifiziernugen}}{\text{gesamte Klassifizierungen}}
\]</div>
<p>In <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> lässt sich die Accuracy wie folgt berechnen:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8235294117647058
</pre></div>
</div>
</div>
</div>
<p>Viel genauer sehen wir jedoch, die korrekt klassifizierten Beobachtungen und Missklassifizierungen anhand einer <strong>Konfusionsmatrix</strong>. Bei der <a href="https://en.wikipedia.org/wiki/Confusion_matrix">Konfusionsmatrix</a> werden die relativen Häufigkeiten <strong>korrekter und inkorrekter Vorhersagen</strong> der vom Modell vorhergesaten Werte und echten Werte gegeneinander in einer Matrix geplotted.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">confusion_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">cm_display</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span>
    <span class="n">confusion_matrix</span><span class="o">=</span><span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">cm_display</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/28d744b288f3eda345167649993d178d639ec0e7023a40bb629c76028f70005a.png" src="../_images/28d744b288f3eda345167649993d178d639ec0e7023a40bb629c76028f70005a.png" />
</div>
</div>
<p>Wir sehen also, das unser Modell ganz gute Vorhersagen liefert, da die meisten Beobachtungen auf der Hauptdiagonalen liegen.</p>
</section>
</section>
</section>
<section id="entscheidungsbaume-vs-lineare-modelle">
<h2>Entscheidungsbäume vs. Lineare Modelle<a class="headerlink" href="#entscheidungsbaume-vs-lineare-modelle" title="Link to this heading">#</a></h2>
<p>Regressions- und Klassifikationsbäume sind eine fundamental andere Herangehensweise an ein Machine Learning Problem im Vergleich zu klassischen Verfahren wie die lineare Regression. Wenn wir uns das lineare Regressiosnmodell ansehen, dann kann es wie folgt formalisiert werden:</p>
<div class="math notranslate nohighlight">
\[
f(X) = \beta_{0} + \sum_{j=1}^p X_{j}\beta_{j}
\]</div>
<p>Im Gegensatz dazu hat ein Entscheidungsbaum folgende Form:</p>
<div class="math notranslate nohighlight">
\[
f(X) = \sum_{m=1}^M c_{m} \cdot 1_{X \in R_{m}}
\]</div>
<p>wobei <span class="math notranslate nohighlight">\(R_{1}, ..., R_{M}\)</span> eine Partition des Prädiktorraumes repräsentiert.</p>
<p>Wenn man sich nun fragt welches der beiden Modelle besser ist, bleibt einem nur festzustellen, dass es auf das Problem ankommt. Wenn der Zusammenhang zwischen den Prädiktoren und der Antwortvariablen gut durch ein lineares Modell aproximiert werden kann, dann wird ein lineares Modell besser als ein Entscheidungsbaum funktionieren. Wenn aber ein non-linearer oder komplexer Zusammenhang zwischen den Prädiktoren und der Antwortvariablen besteht, dann ist ein Entscheidungsbaum besser geeignet. Ein illustratives Beispiel hierzu kann mit folgender Visualisierung gegeben werden:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="c1"># left plot</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">y2</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightblue&quot;</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightcoral&quot;</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;X 1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;X 2&quot;</span><span class="p">)</span>
<span class="c1"># right plot</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mf">0.34</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mf">0.41</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">6.5</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="mf">0.41</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mf">0.77</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">y2</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightblue&quot;</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightcoral&quot;</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;X 1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;X 2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;X 2&#39;)
</pre></div>
</div>
<img alt="../_images/fa6a1e8f4869d0dcf0c029ff25b0780115a0aed22ff745f460eae73bc85ac0cc.png" src="../_images/fa6a1e8f4869d0dcf0c029ff25b0780115a0aed22ff745f460eae73bc85ac0cc.png" />
</div>
</div>
<p>Hier ist auf der linken Seite ein linearer Zusammenhang zwischen x1 und x2 farblich kodiert. Dieser Zusammenhang kann somit auch durch ein lineares Modell wie die lineare Regression beschrieben werden. Ein Entscheidungsbaum auf der rechten Seite hat hier Schwierigekiten, da die Anzahl Regionen sehr hoch sein muss um diese linearen Zusammenhang gut zu beschreiben.</p>
<p>Analog lässt sich das Beispiel auch auf nicht-lineare Zusammenhänge zwischen x1 und x2 veranschaulichen (siehe untenstehende Visualisierung). Hier liefert ein lineares Modell auf der linken Seite keine gute Approximierung, jedoch ein nicht-lineares Modell, wie der Entscheidungsbaum, eine sehr gute (siehe rechte Seite).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="c1"># left plot</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">y2</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightblue&quot;</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">y1</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">y2</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightcoral&quot;</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_betweenx</span><span class="p">(</span>
    <span class="n">y</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span> <span class="n">x1</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">x2</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightblue&quot;</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;X 1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;X 2&quot;</span><span class="p">)</span>
<span class="c1"># right plot</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">y2</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightblue&quot;</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">y1</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">y2</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightcoral&quot;</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_betweenx</span><span class="p">(</span>
    <span class="n">y</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span> <span class="n">x1</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">x2</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightblue&quot;</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;X 1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;X 2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;X 2&#39;)
</pre></div>
</div>
<img alt="../_images/9834078abe1adf94d9b7167a7e17ce8b6cfdc2d610884f89e5be775d65e52e76.png" src="../_images/9834078abe1adf94d9b7167a7e17ce8b6cfdc2d610884f89e5be775d65e52e76.png" />
</div>
</div>
</section>
<section id="vor-und-nachteile-von-entscheidungsbaumen">
<h2>Vor- und Nachteile von Entscheidungsbäumen<a class="headerlink" href="#vor-und-nachteile-von-entscheidungsbaumen" title="Link to this heading">#</a></h2>
<p>Entscheidungsbäume für Regressions- und Klassifikationsprobleme haben einige Vor- und Nachteile gegenüber den Verfahren der vorherigen Kapitel. Diese wollen wir hier auflisten:</p>
<p><strong>Vorteile:</strong></p>
<ul class="simple">
<li><p>Entscheidungsbäume lassen sich sehr leicht erklären und sind intuitiver als andere Verfahren.</p></li>
<li><p>Manche Menschen sind überzeugt, dass Entscheidungsbäume näher an menschlicher Entscheidungsfindung sind als andere Verfahren.</p></li>
<li><p>Eintscheidungsbäume können grafisch dargestellt werden und sind leicht zu interpretieren. Dies ermöglicht auch Nicht-Experten einen Zugang zu den damit untersuchten Problemen.</p></li>
<li><p>Entscheidungbäume können qualitative Prädiktoren vararbeiten ohne Dummy Variablen einführen zu müssen.</p></li>
</ul>
<p><strong>Nachteile:</strong></p>
<ul class="simple">
<li><p>In manchen Fällen können Entscheidungsbäume nicht die selbe Vorhersagegenauigkeit aufweisen wie andere Verfahren, innsbesondere bei linearen Zusammenhängen.</p></li>
<li><p>Entscheidungsbäume können fragil sein gegenüber kleinen Änderungen in den Daten. Das bedeutet das kleine Änderungen an den Daten relativ große Abweichungen in der Vorhersage produzieren können.</p></li>
</ul>
<p>Die soeben angeführten Nachteile lassen sich jedoch durch Erweiterungen der Entscheidungbäume (z.B. durch Bagging, Boosting) kompensieren, was uns auf Entscheidungsbäume angewendet zu <em>Random Forests</em> führen wird. Diesen Themen werden wir uns in den nächsten beiden Kapiteln widmen.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "Mobile-University-DigitalLab/statistik",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Kapitel10"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../Aufgaben/Kapitel09/logregmodell.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Einfaches logistisches Regressionsmodell</p>
      </div>
    </a>
    <a class="right-next"
       href="02_BaggingBoosting.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bagging und Boosting bei Entscheidungsbäumen</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regressions-baume">Regressions-Bäume</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regressionsbaume-in-python">Regressionsbäume in Python</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-pruning">Tree Pruning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regressionsbaume-in-python-und-scikit-learn">Regressionsbäume in Python und Scikit-Learn</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#klassifikations-baume">Klassifikations-Bäume</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#klassifikationsbaume-in-python">Klassifikationsbäume in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#klassifikationsbaume-in-python-und-scikit-learn">Klassifikationsbäume in Python und Scikit-Learn</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entscheidungsbaume-vs-lineare-modelle">Entscheidungsbäume vs. Lineare Modelle</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vor-und-nachteile-von-entscheidungsbaumen">Vor- und Nachteile von Entscheidungsbäumen</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By ixians
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>