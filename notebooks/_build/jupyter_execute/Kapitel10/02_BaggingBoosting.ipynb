{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------\n",
      "Working on the host: imarevic-pc\n",
      "\n",
      "---------------------------------\n",
      "Python version: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0]\n",
      "\n",
      "---------------------------------\n",
      "Python interpreter: /home/imarevic/anaconda3/envs/srh/bin/python\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Load the \"autoreload\" extension\n",
    "%load_ext autoreload\n",
    "# always reload modules\n",
    "%autoreload 2\n",
    "# black formatter for jupyter notebooks\n",
    "# %load_ext nb_black\n",
    "# black formatter for jupyter lab\n",
    "%load_ext lab_black\n",
    "\n",
    "%run ../../src/notebook_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging und Boosting bei Entscheidungsbäumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import statsmodels.api as sm\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im vorherigen Kapitel haben wir zur Reduktion der Varianz das Trimmen oder Pruning von Entscheidungsbäumen kennengelernt. Eine weitere Methode, die unabhängig vom verwendeten Model (Entscheidungsbaum, Regressionmodell, etc.) ist, ist das Bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das <a href=\"https://en.wikipedia.org/wiki/Bootstrap_aggregating\">Bagging</a> ist eine Methode, die durch wiederholtes Sampling der Daten Vorhersagen verbessert. Die Fundamentale Idee stützt sich auf das simple Konzept der Berechnung eines Maßes der zentralen Tendenz (z. B. Mittelwert, Median, Modus), nur dass beim Bagging diese Maße auf Vorhersagen von Modellen angewendet werden:\n",
    "\n",
    "Nehmen wir ein Set der Größe $n$ an **unabhängigen Beobachtungen** an (z.B. mehrere Datensätze der gleichen Art) und bezeichnen diese als $/_{1}, ..., Z_{n}$, wobei jeder Datnesatz die Varianz $\\sigma$ besitzt und den mittelwert $\\hat Z$. Dann ist die Varianz der Mittelwerte $\\hat Z$ gegeben durch $\\sigma²/n$. Anschaulich beduetet dies, dass die Bildung des Mittelwerts über mehrere Beobachtungen/Datensätze die Varianz **reduziert**.\n",
    "\n",
    "Angewendet auf die Nutzung von Machine Learning Modellen, können somit mehrere Modell auf jeweils einer Stichprobe des Datensatzes trainiert werden und die resultierenden Vorhersagen gemittelt werden. \n",
    "\n",
    "Formal:\n",
    "\n",
    "Wir berechnen $\\hat f¹(x), \\hat f²(x), ..., \\hat f^B(x),$ unter Nutzung $B$ verschiedener Trainingsdatensätze. Dann lässt sich ein statistische Model mit geringer Varianz erzeugen durch\n",
    "\n",
    "$$\n",
    "\\hat f_{gemittelt} = \\frac{1}{B} \\sum_{b=1}^B \\hat f^b(x)\n",
    "$$\n",
    "\n",
    "Ind er Praxis stehen uns nicht mehrere unabhängige Trainingsdatensätze zur Verfügung und daher generieren wir aus dem Ursprungsdatensatz durch zufälliges wiederholtes ziehen von Stichproben mehrere **bootsrapped Trainingsdaten** (das ziehen mehrerer Stichproben mit Zurücklegen wird auch <a href=\"https://de.wikipedia.org/wiki/Bootstrapping-Verfahren\">Bootstrap Sampling</a> genannt) um $\\hat f^{*b}(x)$ zu berechnen und final durch Mitteln folgendes Model zu erhalten:\n",
    "\n",
    "$$\n",
    "\\hat f_{bagged} = \\frac{1}{B} \\sum_{b=1}^B \\hat f^{*b}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Verfahren, lässt sich auf jedes statistische Modell anwenden, jedoch findet es im Kontext der Entscheidungsbäume sehr häufig anwendung, da es hier zu besondres guten Modellen führt und die Flexibilität von Entscheidungsbäumen sehr gut ausnutzt. Zum Beispiel können nicht getrimmte Bäume verwendet werden, die sehr tief sind um dennoch mithilfe von Bagging sehr genaue Vorhersagen zu erhalten.\n",
    "\n",
    "Im Falle einer nicht metrischen Vorhersagenvariablen (z.B. im Klassifikationskontext), kann Bagging ebenfalls wie folgt angewendet werden: Man fitted $B$ Klassifikationsbäume an die bootsrapped Trainingsdaten und merkt sich für jedes Model die am besten vorhergesagte Klasse. Aggregiert wird dann indem ein **Majority-Vote** vollzogen wird. Dies bedeutet, dass die finale Vorhersage, die am meisten vorkommende Vorhersage der $B$ Modelle ist.\n",
    "\n",
    "Lassen Sie uns nun zur Veranschaulichung Bagging in Python demonstrieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging für Regressionsbäume in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden nun wieder die **Baseball Hitter-Daten** verwenden um Bagging im Regressionfall zu demonstrieren. Zunächst lesen wir den Datnsatz ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>594</td>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4408</td>\n",
       "      <td>1133</td>\n",
       "      <td>19</td>\n",
       "      <td>501</td>\n",
       "      <td>336</td>\n",
       "      <td>194</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>282</td>\n",
       "      <td>421</td>\n",
       "      <td>25</td>\n",
       "      <td>750.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "5    594   169      4    74   51     35     11    4408   1133      19    501   \n",
       "\n",
       "   CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary NewLeague  \n",
       "1   414     375      N        W      632       43      10   475.0         N  \n",
       "2   266     263      A        W      880       82      14   480.0         A  \n",
       "3   838     354      N        E      200       11       3   500.0         N  \n",
       "4    46      33      N        E      805       40       4    91.5         N  \n",
       "5   336     194      A        W      282      421      25   750.0         A  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../data/hitters.csv\")\n",
    "data = data.dropna()\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes wollen wir zum Vergleich einen Regressionsbaum fitten, der auf den gesamten Daten trainert wurde. Dieser soll als Baseline dienen, gegen die wir dann unser Baggin-Verfahren evaluieren können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enkodierung der Daten\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "#  Features und Target definieren\n",
    "X_df = data.drop(\"Salary\", axis=1)\n",
    "y_df = data[\"Salary\"]\n",
    "X = X_df.values\n",
    "y = y_df.values\n",
    "\n",
    "# Standartisierung der Daten\n",
    "scaler_X = StandardScaler()\n",
    "X_standardized = scaler_X.fit_transform(X)\n",
    "scaler_y = StandardScaler()\n",
    "y_standardized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der MSE auf den Testdaten für einen globalen Regressionsbaum: 1.359673782288005\n"
     ]
    }
   ],
   "source": [
    "# Splitten der Daten in train, validatio und test set\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_standardized, y_standardized, test_size=0.4, random_state=42\n",
    ")\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Fitten des Entscheidungsbaumes\n",
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "y_test_pred = tree.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Der MSE auf den Testdaten für einen globalen Regressionsbaum: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lassen Sie uns nun Bagging mit $B=10000$ anwenden und den finalen Testfehler vergleichen. Wir werden immer 80% der Daten bootsrap samplen, also `samples_fraction = 0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der MSE auf den Testdaten für einen bagged Regressionsbaum: 0.7814575209501579\n"
     ]
    }
   ],
   "source": [
    "B = 10000\n",
    "samples_fraction = 0.8\n",
    "models = []\n",
    "test_preds = []\n",
    "# Training loop\n",
    "n_samples = int(len(X) * samples_fraction)\n",
    "for b in range(1, B + 1):\n",
    "    # ziehe Stichprobe mit Zurücklegen\n",
    "    indices = np.random.choice(len(X_train), size=n_samples, replace=True)\n",
    "    X_sample = X_train[indices]\n",
    "    y_sample = y_train[indices]\n",
    "    # fitte Entscheidungsbäume und speichere jedes Model\n",
    "    tree = DecisionTreeRegressor(random_state=42)\n",
    "    tree.fit(X_sample, y_sample)\n",
    "    models.append(tree)\n",
    "\n",
    "\n",
    "# prediction loop\n",
    "predictions = np.zeros((X_test.shape[0], B))\n",
    "# Erzeuge Vorhersage auf Testdaten und speichere in Liste\n",
    "for i, model in enumerate(models):\n",
    "    predictions[:, i] = model.predict(X_test)\n",
    "\n",
    "# Berechne Mittelwert über alle Vorhersagen\n",
    "bagged_preds = np.mean(predictions, axis=1)\n",
    "\n",
    "# Berechne MSE\n",
    "mse = mean_squared_error(y_test, bagged_preds)\n",
    "print(f\"Der MSE auf den Testdaten für einen bagged Regressionsbaum: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen also, dass der Testfehler geringer ausfällt und wir somit ein besseres Model zur Vorhersage durch Bagging erhalten haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging für Klassifikationsbäume in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden wir der Vollständigkeit halber Bagging auch auf Klassifikationsbäume anwenden. Die Implementierung ist im Prinzip identisch zum Regressionsfall. Wir werden analog zum vorherigen Kapitel nun ebenfalls den **Hurricane-Datensatz** verwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>FirstLat</th>\n",
       "      <th>FirstLon</th>\n",
       "      <th>MaxLat</th>\n",
       "      <th>MaxLon</th>\n",
       "      <th>LastLat</th>\n",
       "      <th>LastLon</th>\n",
       "      <th>MaxInt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RowNames</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>430</td>\n",
       "      <td>NOTNAMED</td>\n",
       "      <td>1944</td>\n",
       "      <td>1</td>\n",
       "      <td>30.2</td>\n",
       "      <td>-76.1</td>\n",
       "      <td>32.1</td>\n",
       "      <td>-74.8</td>\n",
       "      <td>35.1</td>\n",
       "      <td>-69.2</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>432</td>\n",
       "      <td>NOTNAMED</td>\n",
       "      <td>1944</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>-74.9</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-78.1</td>\n",
       "      <td>32.6</td>\n",
       "      <td>-78.2</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>433</td>\n",
       "      <td>NOTNAMED</td>\n",
       "      <td>1944</td>\n",
       "      <td>0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>-65.2</td>\n",
       "      <td>16.6</td>\n",
       "      <td>-72.2</td>\n",
       "      <td>20.6</td>\n",
       "      <td>-88.5</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>436</td>\n",
       "      <td>NOTNAMED</td>\n",
       "      <td>1944</td>\n",
       "      <td>0</td>\n",
       "      <td>20.8</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>-72.3</td>\n",
       "      <td>42.1</td>\n",
       "      <td>-71.5</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>437</td>\n",
       "      <td>NOTNAMED</td>\n",
       "      <td>1944</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-84.2</td>\n",
       "      <td>20.6</td>\n",
       "      <td>-84.9</td>\n",
       "      <td>19.1</td>\n",
       "      <td>-93.9</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Number      Name  Year  Type  FirstLat  FirstLon  MaxLat  MaxLon  \\\n",
       "RowNames                                                                     \n",
       "1            430  NOTNAMED  1944     1      30.2     -76.1    32.1   -74.8   \n",
       "2            432  NOTNAMED  1944     0      25.6     -74.9    31.0   -78.1   \n",
       "3            433  NOTNAMED  1944     0      14.2     -65.2    16.6   -72.2   \n",
       "4            436  NOTNAMED  1944     0      20.8     -58.0    26.3   -72.3   \n",
       "5            437  NOTNAMED  1944     0      20.0     -84.2    20.6   -84.9   \n",
       "\n",
       "          LastLat  LastLon  MaxInt  \n",
       "RowNames                            \n",
       "1            35.1    -69.2      80  \n",
       "2            32.6    -78.2      80  \n",
       "3            20.6    -88.5     105  \n",
       "4            42.1    -71.5     120  \n",
       "5            19.1    -93.9      70  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hurricanes = pd.read_excel(\"../../data/hurricanes.xlsx\", index_col=0)\n",
    "hurricanes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benamung der Origin\n",
    "hurricanes[\"Origins\"] = hurricanes[\"Type\"].replace(\n",
    "    {0: \"tropisch\", 1: \"aussertropisch\", 3: \"aussertropisch\"}\n",
    ")\n",
    "# Erstellung von X und y Datensätzen\n",
    "X = hurricanes[\"FirstLat\"].values\n",
    "X = sm.add_constant(X)\n",
    "y = hurricanes[\"Origins\"].replace({\"tropisch\": 0, \"aussertropisch\": 1}).values\n",
    "\n",
    "# test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun haben wir Test und Trainingsdaten und können mit der Implementierung der Bagging Variante des Klassifikaitonsbaumes fortfahren. Die Implementierung sieht wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 10000\n",
    "samples_fraction = 0.8\n",
    "models = []\n",
    "test_preds = []\n",
    "# Training loop\n",
    "n_samples = int(len(X) * samples_fraction)\n",
    "for b in range(1, B + 1):\n",
    "    # ziehe Stichprobe mit Zurücklegen\n",
    "    indices = np.random.choice(len(X_train), size=n_samples, replace=True)\n",
    "    X_sample = X_train[indices]\n",
    "    y_sample = y_train[indices]\n",
    "    # fitte Entscheidungsbäume und speichere jedes Model\n",
    "    tree = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n",
    "    tree.fit(X_sample, y_sample)\n",
    "    models.append(tree)\n",
    "\n",
    "\n",
    "# prediction loop\n",
    "predictions = np.zeros((X_test.shape[0], B), dtype=int)\n",
    "# Erzeuge Vorhersage auf Testdaten und speichere in Liste\n",
    "for i, model in enumerate(models):\n",
    "    predictions[:, i] = model.predict(X_test)\n",
    "\n",
    "# Berechne Majority Vote über alle Vorhersagen\n",
    "bagged_preds = np.array(\n",
    "    [np.bincount(predictions[i]).argmax() for i in range(predictions.shape[0])]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir berechnen auch hier die Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Accuracy Score auf den Testdaten für einen bagged Klassifikationsbaum: 0.8235294117647058\n"
     ]
    }
   ],
   "source": [
    "# Berechne Accuracy\n",
    "acc = accuracy_score(y_test, bagged_preds)\n",
    "print(\n",
    "    f\"Der Accuracy Score auf den Testdaten für einen bagged Klassifikationsbaum: {acc}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genauso wie das Bagging, kann <a href=\"https://de.wikipedia.org/wiki/Boosting\">Boosting</a> generell bei allen machine leanring Verfahren eingesetzt werden. Es ist vom Prizip her nicht auf Entscheidungsbäume beschränkt, da es aber bei einer Variante der Random Forests zum Einsatz kommt und somit im Kontext der Entscheidungsbäume sehr verbreitet ist, werden wir Boosting ebenfalls unter Verwendung von Entscheidungsbäumen vorstellen. \n",
    "\n",
    "Wir erinnern uns, dass beim Bagging das *Bootstrap Sampling* zum Einsatz kam um für jedes Model eine neue Stichprobe zu ziehen und die Vorhersagen am Ende global zu aggregieren. Im Gegensatz hierzu involviert das Boosting nicht wiederholtes Ziehen eines Bootsrap Samples, sondern es ist ein **sequentielles Verfahren**, dass Informationen von zuvor aufgespannten Entscheidungsbäume in das Lernverfahren einbezieht und somit jeder Entscheidungsbaum auf einer modifizierten Version der Originaldaten trainiert wird.\n",
    "\n",
    "Im Gegensatz zum Bagging ist das Boosting ein **langsames Verfahren**. Es wird so bezeichnet, da hier ein Entscheidungsbaum zunächst auf die Daten gefittet wird und danach immer wieder auf die Residuen der vorhersagen des vorherigen Models. Es werden also zu jeder Iteration die **Residuen** als $Y$ betrachte und ein Model darauf trainiert. Der resultierende Baum wird dann nach jeder Iteration zurück in die gefittete Funktion integriert um die Residuen zu updaten. Die verwendeten Bäume können hierbei sehr klein sein. Beim Boosting wird diese Tiefe durch den Parameter $d$ kontrolliert. Durch Nutzung sehr kleiner Bäume, verbessern wir die Vorhersage sehr *langsam* und Verbessern somit die finale Funktion $\\hat f$, die das Gesamtmodel über die Iterationen beschreibt besonders in den Bereichen, in denen das Model schlechte Vorhersagen macht. Das sehr langsame Lernen wird noch weiter verlangsamit, wenn wir einen Parameter $\\lambda$ einbauen, der die Lernrate verringert und somit erlaubt verschiedene Bäume aufzubauen, da insgesamt mehr Bäume möglich sind.\n",
    "\n",
    "Formal lässt sich der Boosting-Algorithmus wie folgt beschreiben:\n",
    "\n",
    "1. Setze $\\hat f(x)=0$ und $r_{i} = y_{i}$ für alle Beobachtungen $i$ im Trainingsdatensatz.\n",
    "2. Für $b =1,2, ..., B$, wiederhole folgendes:\n",
    "    \n",
    "    A. Fitte einen Baum $\\hat f^b$ mit $d$ Splits (d+1 finale Knoten) an die Trainingsdaten.\n",
    "    \n",
    "    B. Update $\\hat f$ durch hinzufügen einer kleineren Version des vorherigen Baumes:\n",
    "    \n",
    "$$\n",
    "\\hat f(x) \\leftarrow \\hat f(x) + \\lambda \\hat f^b(x)\n",
    "$$\n",
    "    \n",
    "    C. Update die Residuen r:\n",
    "    \n",
    "$$\n",
    "r_{i} \\leftarrow r_{i} - \\lambda \\hat f^b(x_{i})\n",
    "$$\n",
    "\n",
    "\n",
    "3. Gebe das geboostete Model zurück:\n",
    "\n",
    "$$\n",
    "\\hat f(x) = \\sum_{b=1}^B \\lambda \\hat f^b(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im generellen Boosting-Algorithmus sind folgende Parameter involviert:\n",
    "    \n",
    "\n",
    "|  Parameter | Beschreibung  |\n",
    "|---|---|\n",
    "|  B | Die Anzahl an Bäumen. Wenn B zu groß gewählt wird, kann eine Boosting-Agorithmus overfitten. Daher wird B häufig durch Krossvalidierung auf einem Validierungsdatensatz gewählt  |\n",
    "|  $\\lambda$  | Dieser Parameter wird **Shrinkage Patameter** genannt, da dieser die Lernrate kontrolliert, mit der der Boosting-Algorithmus lernt. Er sorgt dafür dass die Bäume sich über mehrere Iterationen nicht zu sehr ähneln.|\n",
    "| $d$ | Dieser Parameter kontrolliert die Komplexität der Bäume. Beim Boosting werden häufig nur sehr geringe Tiefen gewählt. Meistens reicht eine Tiefe von $d=1$ vollkommen aus (nur 1 Split).|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python lässt sich Boosting bei Entscheidungsbäumen gut mit der Bibliothek `scikit-learn`implementieren. Hierbei ist zu beachten, dass verschieden Varianten des oben beschriebenen Boosting-Algorithmus existieren:\n",
    "\n",
    "- AdaBoost (Adaptive Boosting)\n",
    "- Gradient Boost\n",
    "- XGBoost\n",
    "\n",
    "Jede dieser Varianten. Zum Beispiel werden bei AdaBoost die Gewichte des Models bei jeder Iteration geupdatet und bei Gradient Boosting wird oben bschriebens Udate Verfahren über die Residuen gewählt. XGBoost ermöglicht noch weitere Regularizierung des Models. Die Details jedes dieser Verfahren sind jedoch nicht Gegenstand dieses Moduls.\n",
    "\n",
    "Im Folgenden werden wir anschaulich Gradient Boosting aus `scikit-learn` in Python verwenden. Wir nutzen hierfür wieder den Baseball Hitter Datensatz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>594</td>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4408</td>\n",
       "      <td>1133</td>\n",
       "      <td>19</td>\n",
       "      <td>501</td>\n",
       "      <td>336</td>\n",
       "      <td>194</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>282</td>\n",
       "      <td>421</td>\n",
       "      <td>25</td>\n",
       "      <td>750.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "5    594   169      4    74   51     35     11    4408   1133      19    501   \n",
       "\n",
       "   CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary NewLeague  \n",
       "1   414     375      N        W      632       43      10   475.0         N  \n",
       "2   266     263      A        W      880       82      14   480.0         A  \n",
       "3   838     354      N        E      200       11       3   500.0         N  \n",
       "4    46      33      N        E      805       40       4    91.5         N  \n",
       "5   336     194      A        W      282      421      25   750.0         A  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../data/hitters.csv\")\n",
    "data = data.dropna()\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enkodierung der Daten\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "#  Features und Target definieren\n",
    "X_df = data.drop(\"Salary\", axis=1)\n",
    "y_df = data[\"Salary\"]\n",
    "X = X_df.values\n",
    "y = y_df.values\n",
    "\n",
    "# Standartisierung der Daten\n",
    "scaler_X = StandardScaler()\n",
    "X_standardized = scaler_X.fit_transform(X)\n",
    "scaler_y = StandardScaler()\n",
    "y_standardized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Splitten der Daten in train, validatio und test set\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_standardized, y_standardized, test_size=0.4, random_state=42\n",
    ")\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun werden wir den `GradientBoostingRegressor()` auf die Trainisdaten anwenden und den MSE auf dem Testdatensatz ausgeben. Es ist zu bacten, dass wir im Dictionary `params` ein paar **Hyperparameter** setzen, die das Training kontrollieren. In unserem Beispiel wählen wir die verbreiteten Settings (z.B. `n_estimators : 500` oder `learning_rate : 0.01`) und erlauben nur sehr keine Bäume (`mas_depth : 4`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) auf den Testdaten: 0.8217\n"
     ]
    }
   ],
   "source": [
    "# Setzen der Lernparameter\n",
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "\n",
    "# Modelfitting\n",
    "reg = GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# MSE\n",
    "mse = mean_squared_error(y_test, reg.predict(X_test))\n",
    "print(\"The mean squared error (MSE) auf den Testdaten: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu veranschaulichen wie der Boosting-Algorithmus **langsam** auf den Traininsdaten lernt und bei jeder Iteration die **vorherige Vorhersage etwas verbessert**, werden wir nun den Trainings- und Testfehler für jede Iteration $B = 1, 2, ..., N$ plotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_score = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\n",
    "for i, y_pred in enumerate(reg.staged_predict(X_test)):\n",
    "    test_score[i] = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title(\"Trainings- vs. Testdaten Fehler\")\n",
    "plt.plot(\n",
    "    np.arange(params[\"n_estimators\"]) + 1,\n",
    "    reg.train_score_,\n",
    "    \"b-\",\n",
    "    label=\"Trainingsset Fehler\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(params[\"n_estimators\"]) + 1, test_score, \"r-\", label=\"Testset Fehler\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting Iterationen\")\n",
    "plt.ylabel(\"Fehler\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Abschluss lässt sich Bagging und Boosting wie folgt zusammenfassen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bagging_vs_boosting](../_img/bagging_vs_boosting.png \"Bagging vs. Boosting\")\n",
    "\n",
    "> **Bagging**:<br>\n",
    "> - Ziehe N Trainingsdatensätze mit Zurücklegen\n",
    "> - Trainiere auf jedem der Datensätze ein Modell\n",
    "> - Aggregiere die Vorhersagen der Modelle um eine genauere finale Vorhersage zu erhalten\n",
    "\n",
    "> **Boosting**:<br>\n",
    "> - Trainiere ein Modell auf den Trainingsdaten\n",
    "> - Fitte das Model auf die Residuen des vorherigen Trainingslaufs und update die Residuen\n",
    "> - Füge bei jeder Iteration sequentiell das erhaltene Model dem Ursprungsmodel hinzu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}